[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FWF 610",
    "section": "",
    "text": "Welcome!",
    "crumbs": [
      "Home",
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#syllabus",
    "href": "index.html#syllabus",
    "title": "FWF 610",
    "section": "Syllabus",
    "text": "Syllabus\nPlease check Canvas for an updated syllabus.",
    "crumbs": [
      "Home",
      "Welcome!"
    ]
  },
  {
    "objectID": "index.html#objectives-and-structure",
    "href": "index.html#objectives-and-structure",
    "title": "FWF 610",
    "section": "Objectives and structure",
    "text": "Objectives and structure\nSpecific Course Objectives\nThe objectives of this class are multiple.\nFirst, this course is designed to review basic probability and statistics (Module 1).\nSecond, develop your understanding and ability to develop models that are usual in research in agriculture and natural resources.\nThird, develop data management skills.\nFourth, develop coding expertise for statistical analysis and for other tools.\nFifth, develop the ability to design great publication ready plots in R. Finally, this course will cater to the students research needs, and some topics may be added in case there is a need.",
    "crumbs": [
      "Home",
      "Welcome!"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Welcome to week 1\nThis section will introduce you to\nWe will also discuss multiple important concepts!\nBefore we get into the weeds of data analysis, coding, and R, we should remember some basics! On Friday we will learn (or refresh) some basic R skills! If you have a lot of experience in R, just hang in there, we will get more and more complex topics as we go on! If you’re brand new, also hang in there, this may seem overwhelming and very time consuming. It will get easier!\nWe will figure out theory and coding as we go. For all assignments, we will use Quarto. It allows you to write some neat reports while using code and plots.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "intro.html#welcome-to-week-1",
    "href": "intro.html#welcome-to-week-1",
    "title": "Introduction",
    "section": "",
    "text": "Important\n\n\n\nIf you are reading this file before class, it may not make a lot of sense to you. That’s OK! We will go through it on the first class. More importantly, you will learn more about R on Friday.\n\n\n\n\n\nR Projects\nWe will learn more about projects during class. We will have our first R assignment this Friday. For the time being, just trust me, and let’s do the following:\n\nOpen R Studio, go to file &gt; new project &gt; new directory &gt; new project\nName it SNR610, and use a directory that makes sense to you\nDownload the SNR610_W1_notes.qmd and the .csv files from CANVAS IN THE NEWLY CREATED PROJECT DIRECTORY\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you are working on this before class, you can (and should) stop now\n\n\n\n\nStatistics refresher\nOpen R and make sure SNR 610 project is opened. Then open the SNR610_W1_notes.qmd file in R. We will continue there!\nFirst of, we will discuss some topics.\nI want you to take notes in the .qmd document\nWhat is statistics?\nWhat is a population and a sample? Give examples\nData\nVariable and observation\nTypes of data:\nCategorical: Nominal and Ordinal\nNumerical: Discrete and Continuous\nNow, let’s go back to the html file and follow the instructions to explore some different types of data.\n\n\nData exploration\nDownload and read the butterfly file.\n\nIf you are comfortable with R basics, you can write the code for this exercise directly into Quarto. To add extra code chunks, write this symbols: ```{r}, if you are not comfortable with R, I recommend for the time being you open a new file and copy and paste the code there, and run it}\n\nTo read data, we use the following line:\n\ndata&lt;-read.csv(\"data/ButterflyData.csv\", stringsAsFactors = T)\n\nThis only works if the file is in the current working directory. Make sure you download the file to the projects folder!\nLet’s look at some of the data, the head command shows us the top 5 rows:\n\nhead(data)\n\n  X Species Status Nparasites ForewingArea\n1 1 Viceroy      1          2     964.8420\n2 2 Monarch      3          0    1064.9463\n3 3 Viceroy      2          2    1087.6290\n4 4 Viceroy      2          3    1025.4572\n5 5 Monarch      2          1     986.7462\n6 6 Viceroy      2          2    1029.7973\n\n\nSo, we have data on three types of butterflies: Viceroy, Monarch, and Queen:\n\n\n\nThe three types of Butterflies that we have data on.\n\n\nWe have data on forewing area, number of parasites, species, and status. For status we have numbers 1, 2, and 3, which represent the following:\n1: poor\n2: OK\n3: Good\nWhat types of variables are there? Identify each variable type. What are the variables and what are the observations?\nLet’s look at the structure of the file:\n\nstr(data)\n\n'data.frame':   134 obs. of  5 variables:\n $ X           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Species     : Factor w/ 3 levels \"Monarch\",\"Queen\",..: 3 1 3 3 1 3 1 3 3 1 ...\n $ Status      : int  1 3 2 2 2 2 3 2 2 2 ...\n $ Nparasites  : int  2 0 2 3 1 2 1 4 1 3 ...\n $ ForewingArea: num  965 1065 1088 1025 987 ...\n\n\nNote that this data is a bit messy. We will learn how to clean data during the semester. But this is closer to what your data will look like initially!\nWhat is a factor?\nShould any other variable be a factor?\n\ndata$Status&lt;-as.factor(data$Status)\n\nWhat is a factor?\n\nstr(data)\n\nYou can look at the whole dataset using the following:\n\nprint(data)\n\nLook at the data. Do you think there is an effect of # of parasites on wing growth? Do you think different species have different sizes? How can we tell?\n\n\nInferential statistics\nWe can use inferential statistics to answer these questions! But why? What are inferential statistics, and what is special about inferential statistics? Discuss\nThoughts:\nMore on that next week.\n\n\nPlotting\nLook at the data. Usually the best exploratory analysis is a visual one. We will learn about plotting during the course. We will learn about good plots, bad plots, and how to use R and ggplot to make some publication level plots.\nFor the purposes of this exercises, I want you to think about what type of plot you ant to make, and attempt to make it. You can use R, you can use excel, you can try to draw it. You can work in teams, but try to work mostly by yourself. We will revisit this dataset at the end of the course, and you will plot it again. Hopefully this will show some of the learning you have obtained!\n\n\nProbability\nTo understand inferential statistics, we need to understand probability.\nDiscuss and write a definition for probability:\nWhat is a probability distribution? Give one example\n\n\nThis is your file now!\nGo to the top of the file, and replace the “author” name for yours. Try to Render the file, it will hopefully work and have all your notes :)\n\n\nFriday\nIntroduction to R, Projects, and Quarto.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "Assignment1.html",
    "href": "Assignment1.html",
    "title": "1  Intro to R, Projects, and Quarto",
    "section": "",
    "text": "1.1 Introduction to R\nIntroduction to R, Projects, and Quarto.\nNext week topic will be:\nR is probably the most used language for statistical analysis in natural resources and agriculture. Other people use SAS (still very popular, but it is slowly becoming more unpopular) and python. R is designed for statistical analysis and data visualization, and in my opinion it is more intuitive, cleaner and faster than python for a majority of general uses in data analysis. It is not the only option, but it is an incredible resource for data analysis.\nR is also free and open source, and there are incredibly smart people constantly working on developing packages that help you with your specific needs.\nMore importantly, whether you are going to continue your academic career, go into the professional world, or analyze your data the use of R is now a huge advantage in the job market!\nThe objectives for this course (R-wise) are the following:",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R, Projects, and Quarto</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#introduction-to-r",
    "href": "Assignment1.html#introduction-to-r",
    "title": "1  Intro to R, Projects, and Quarto",
    "section": "",
    "text": "Learn the logic and syntax of coding! This is the most important aspect. If you understand the logic, then you can run whatever you want. It might take some research, looking for some functions, asking online, but you can get there\nInterpret results. Particularly for linear models and generalized linear models. Are the results significant? What is the effect size? What is the effect of the independent variable on the dependent variable? What do I report on my paper? All of these questions and more\nUnderstand error messages. Why is there an error message, what does it mean, and how to fix it. Errors in your code are unavoidable, and while often times it’s easy to find solutions online, sometimes it isn’t, and it is always better when you can understand them yourself.\nGet better and more efficient at it. Don’t spend hours wrangling or moving data around before you can even attempt to analyze it.\nUse the great visualization tools available to make great publication-quality plots\n\n\n1.1.0.0.1 Let’s get started!\nPlease contact me (or raise your hand if in the classroom) to get help.\nOpen R-Studio and go to file &gt; new script.\nThis is usually how we start a new script where we will be writing code\nAt this point, you should see four panes:\n\n\n\nFour panes\n\n\nMine looks a bit different, but it is the same logic:\n\n\n\nThis is how my panes look\n\n\nThe different panes do different things. Let’s go one by one on what they do:\n\n\n1.1.0.0.2 Console\nThis is what the console looks like:\n\n\n\nConsole\n\n\nYou can type code here, and this is where the outputs are “printed”. Notice in the figure how I wrote 1+1 and it gave me a result? Try it yourself. Try to use this “calculator” and multiply and divide.\n\n\n\n\n\n\nWork at your own pace!\n\n\n\nI want you to worry less about grades, and more about learning and acquiring skills.\nIf you have used R before, some of the steps in this assignment mat be too “basic”. If that’s the case feel free to skip them. Also, feel free to help other people that may be having issues or struggling. Collaborative learning can be super useful. And teaching/helping allows you to acquire even deeper knowledge.\nAt the same time, if you are brand new and are struggling, and this is taking a while to complete. Feel free to not do the last parts of the assignment (just let me know what you had issues with). Hopefully later in the course you won’t feel the need to do this.\nGraduate student life can be very busy. I want you to spend as much time as you can learning R and stats, because I believe it can be extremely beneficial for your career, but DO NOT sacrifice your learning in other areas (particularly research). Talk to me if you need more time, or have any issues\n\n\nTry to run the following lines of code (if you are experienced with all of these commands, you can skip this step). You can copy and paste the code. Run them one by one, and try to understand what is happening:\n\n5+5\n\n\n8^5\n\n\nsqrt(9)\n\n\nc(5,7,9,15,50)\n\n\nmean(c(5,7,9,15,50))\n\n\nsd(c(5,7,9,15,50))\n\n\n1:10\n\n-5:10\n\nc(1:10,-5:10,25)\n\nThese are all basic commands!\n\n\n1.1.0.0.3 Source\nNow that you got familiar with what the console can do, it’s time to remember the following: DO NO WRITE YOUR CODE DIRECTLY INTO THE CONSOLE! Using it as a quick calculator may be OK (as we just did), but for the majority of your work and analyses, you want to write scripts, and write your code there.\nThese scripts are written on the source window, which looks like this:\n\n\n\nSource window\n\n\nHere is where you can write all of your code and script. This is a code editor, so, we can re-run lines of code and modify/edit them for future use.\nTry writing all of the code chunks you just ran, but instead of doing it in the console, write them in source window. To run each line of code you can use ctrl + Enter (at least on Windows, I am sure there must be a shortcut for macOS). You can also click run on top right corner of the window.\n\n\n1.1.0.0.4 Creating and naming objects. Also; the environment and history pane\nSo far, you have been running r just as a calculator, but the strength of R is in running full scripts, models, and analyses. To do so, we need to use R less as a calculator and more as a programming software.\nTo create objects in R we use the &lt;- operator. This is probably the most important operator in R. While you can create objects with = I would avoid this, as it can create some issues later on. This is how you create and name objects:\n\\[\n\\underbrace{x}_{name} \\underbrace{&lt;-}_{symbol} \\underbrace{17}_{value}\n\\]\nHere, we created an object called x, with a value of 17.\nTry writing x &lt;- 17 in your script/code editor (not in the console!) and run it.\nNow, let’s create a new object called y\n\ny&lt;- x*2\n\nAgain run that.\nAs you can see, R is running these lines, but it is not printing the values.\nIf you look at the environment/history pane, you can see all of the objects that are loaded in R. This should include x and y and their values. You can also see their values by running the following code:\n\nprint(x)\n\nprint(y)\n\nAs you probably also noticed, y equals 34. This is because \\(y = 2x\\) and \\(x = 17\\). This is super useful! You don’t need to use numbers every time, you can also use objects.\nYou can also print objects by simply running y or x:\n\ny\n\n\n\n1.1.0.1 Vectors\nYou can create vectors in different ways.\nWay 1:\n\nvector1&lt;-c(4,6,1,11,50)\n\nRun that and check the values of the vector. Does it make sense?\nVectors can be numeric, or other types:\n\nBreed&lt;-c(\"Holstein\",\"Hereford\",\"Longhorn\",\"Longhorn\",\"Longhorn\",\"Hereford\")\n\nWe can ask R about the class of an object.\n\nclass(vector1)\n\nclass(Breed)\n\nWe can also change classes:\n\nBreed&lt;-(as.factor(Breed))\nBreed\nclass(Breed)\n\nHere we changed from character to factor. See how it created a “level”? Essentially, R is making 3 groups and assigning each entry in the vector to one of the groups (that’s a good way to think about it). It makes a lot of computational sense, and it is needed for modeling.\nNow, let’s try something fun. Let’s go back to numeric vectors.\nIf you remember, x = 17 and y = 34. Run the following:\n\ny-vector1\n\nWhat happened when you did that? You are doing vector math! This can be incredibly useful!\nAnother way of creating a vector:\n\nvector2&lt;-1:100\nvector3&lt;-1001:1100\nprint(vector2)\nprint(vector3)\n\nRun those lines, does it make sense?\nIf we want to extract a specific value, we can use [] as an index.\nFor example:\n\nvector2[5]\n\nGives us the 5th value on vector2, which turns out is 5\nAlso, try the following:\n\nvector3[5]\n\nWhich should give us the 5th value of vector3: 1005.\nNow, try the following:\n\nvector2+vector3\n\nWhat happened? This is called vectorized math. Essentially, the first number in vector 2 was added to the fist number in vector 3, and then the second number on vector 2 was added to the second number on vector 3 and so on. This will be super useful for you!\nThere is another way of creating a vector:\n\nvector4&lt;-seq(1,100,4)\n\nThat can also be pretty useful! Do you understand what happened? You are creating a sequence of numbers! From 1\nActually, there are many, many, many ways of creating vectors. Which are simply a one dimensional matrix. Here are some examples that I use:\n\nrnorm(50,0,5) #We sample 50 random values from a normal distribution with mean 0 and sd 5\nrpois(100,10) #We sample 100 random values from a poisson distribution with lamba 10\nrunif(100,-10,10) #We sample 100 random individuals from a uniform distribution from -10 to 10\ndpois(1:20,5) # The probability of obtaining a value of 1, 2, 3, ... ,20 if we were sampling randomly from a Poisson distribution.\n\nWe won’t have much time to go over probability distributions. If you are unfamiliar with them, as part of the class, the first reading will be 🔗 chapter 4🔗 and 🔗chapter 5🔗 of 🔗An Intuitive, Interactive, Introduction to Biostatistics by Caitlin Ward and Collin Nolte🔗 . I would recommend you read those chapters before continuing (Ward and Nolte 2024)\n\n\n\n\n\n\nNote\n\n\n\nIf you can’t figure out what a “function” does. Using the help function might help. Use ? to ask R about a function. Try running ?rnorm and see if you can figure out what this function does.\nAgain, no problem if you can’t figure it out. By the end of the semester, you will! Also, what even is a function anyway?\n\n\n\n\n1.1.1 Functions\nR comes with tons of pre-built functions. You can also build your own, which can be a great way to do an analysis that you have to repeat multiple times. There are also packages that you can download to analyze your data. Essentially, a function is a code chunk that performs a task. However, it does require you to give it some information. That information is called input. Let’s go back to the runif function. This function generates random deviates following a uniform distribution. However, to do that, it needs some information. The information that it needs is: runif(n, min, max) n is the number of deviates you want to generate. Min is the minimum potential value, and max is the maximum potential value (the limits of the uniform distribution. Try it:\n\nrunif(n=50, min=-20,max=20)\n\nLet’s try using rnorm(n,mean,sd) which generates n random variates using a normal distribution with mean “mean”, and standard deviation “sd”. Try it:\n\nrnorm(20,0,1) \n\nIf you notice, we did not have to specify that 20 was n, 0 was the mean and 1 was the sd. R knew it already. inputs in functions have a pre-determined order.\nWe will explore a lot more functions in the next section.\n\n\n1.1.2 Matrices and Data Frames\nOur data usually have a different structure than just a single vector. We usually have excel files, databases, and dataframes with multiple columns and multiple rows. We will mostly work with dataframes during the course. Later on we will incorporate lists, and matrices with more than 2 dimensions though.\nMatrices and dataframes in r have different characteristics. An important one is that Matrices in R CANNOT contain different types of data. They all have to be either numerical, character, or logical or other. While matrices are super useful, we will focus on using dataframes first. Dataframes can contain multiple types of data.\nNow, let’s work on some real data.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R, Projects, and Quarto</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#reading-data-into-r",
    "href": "Assignment1.html#reading-data-into-r",
    "title": "1  Intro to R, Projects, and Quarto",
    "section": "1.2 Reading data into R",
    "text": "1.2 Reading data into R\nYou can import data of various formats into R; they include data tables in the form of .dbf, .csv, and .xlsx files or even spatial data such as vectors .shp or rasters .nc.\nBut the most common type of data files imported into R are probably.csv files.\nYou can import the dataset using the function read.csv()\nType this into R:\n\nread.csv('C:/Users/amolina6/Documents/projects/FWF690/ButterflyData.csv')\n\nThe text within the brackets, and contained in quotation marks, is the file path (directory and file name) of the file you want to import. This is my directory. Yours would most likely be different. Perhaps something like…\nIF MAC:\n\nread.csv('~/FWF690/ButterflyData.csv')\n\nIF WINDOWS:\n\nread.csv('C:/Users/YourUserID/FWF690/ButterflyData.csv')\n\nHow do you find the file path?\nFor mac, open Finder, click to the folder where you saved your data, find your file, right-click (or ctrl-click) it, and then click on GetInfo.\nFor Windows, you can search documents, or you can look at the latest downloads on your browser.\nWhen we read a datafile, we need to create an “R object”. We can do it this way:\n\\[ \\underbrace{data}_{Object\\; name} \\; \\; \\; \\underbrace{&lt;-}_{arrow} \\; \\; \\underbrace{read.csv('~C:/.../eel.csv')}_{Data} \\]\nYou can also do the following to import data into r:\n\nYou can change your working directory, to the folder where your data is located, and then you don’t have to write the flepath. Check this website: https://dzchilds.github.io/eda-for-bio/working-directories-and-data-files.html (Childs 2024)\nCreate a project. If you create a project, the location of said project (folder) will be the working directory. If you download files directly into the folder, you can read them without specifying a working directory.\n\nI highly recommend you create a project. This will make your life easier and that’s what I do. Make sure to go to file &gt; new project &gt; new directory, and select a location in your computer that will be easy for you to access. At this point I would recommend you download ALL files directly into the project folder. Again, this is what I do, so I read files like this:\n\ndata&lt;-read.csv(\"data/ButterflyData.csv\", stringsAsFactors = T)\n# raise your hand if you are  struggling to read this.\n\nThis file will only open if it’s saved in the same working directory where you are currently working or in the same project.\n\n\n\n\n\n\nTip\n\n\n\nDid you realize the # symbol in that last code chunk? This is a very useful symbol. It is used to comment the code, and everything after the symbol won’t run. It’s a good idea to comment all of your scripts.\n\n\nGoing back to our dataset, head shows us the top rows:\n\nhead(data)\n\n  X Species Status Nparasites ForewingArea\n1 1 Viceroy      1          2     964.8420\n2 2 Monarch      3          0    1064.9463\n3 3 Viceroy      2          2    1087.6290\n4 4 Viceroy      2          3    1025.4572\n5 5 Monarch      2          1     986.7462\n6 6 Viceroy      2          2    1029.7973\n\n\nWe can use summary to obtain a summary of the dataset:\n\nsummary(data)\n\n       X             Species       Status        Nparasites    \n Min.   :  1.00   Monarch:41   Min.   :1.000   Min.   : 0.000  \n 1st Qu.: 34.25   Queen  :49   1st Qu.:1.000   1st Qu.: 0.000  \n Median : 67.50   Viceroy:44   Median :2.000   Median : 2.000  \n Mean   : 67.50                Mean   :2.075   Mean   : 2.246  \n 3rd Qu.:100.75                3rd Qu.:3.000   3rd Qu.: 3.000  \n Max.   :134.00                Max.   :3.000   Max.   :10.000  \n  ForewingArea   \n Min.   : 806.5  \n 1st Qu.: 949.4  \n Median :1027.7  \n Mean   :1022.7  \n 3rd Qu.:1087.1  \n Max.   :1209.6  \n\n\nWe can also ask it how many columns and how many rows it has. This is a great way to “count” observations. We use ncol() and nrow() to do this:\n\nnrow(data)\n\n[1] 134\n\n\n\n1.2.0.1 Indices in dataframes\nWe can index dataframes using [,], and you should think of it as [rows,columns] or, in the case of most dataframes: [observation,variable]. So, [5,] is the fifth observation:\n\ndata[5,]\n\n  X Species Status Nparasites ForewingArea\n5 5 Monarch      2          1     986.7462\n\n\nWhile [,5] is the fifth variable, in this case “Forewing Area”:\n\ndata[,5]\n\n  [1]  964.8420 1064.9463 1087.6290 1025.4572  986.7462 1029.7973 1092.1279\n  [8] 1030.1300  951.0215  952.0203  972.7687  923.5299  965.2786 1067.0532\n [15]  806.4759  933.6333  984.8791  900.8961  896.5896 1073.4148 1031.5598\n [22] 1122.3702  923.2774 1010.8952 1183.9889 1085.7107 1159.4511 1023.0000\n [29]  861.2154  906.4139 1046.9687 1165.4515 1037.9212  900.6176 1093.4250\n [36] 1050.7947  922.8024  929.6439 1045.5846  832.4090  984.0448 1059.0467\n [43]  886.6116  928.6071  992.8063 1117.8374 1012.8004 1057.8563 1174.0423\n [50] 1078.7093  982.5831 1141.2504 1117.7142 1049.4940  945.7766 1193.0925\n [57]  993.9152  900.3906  977.3681 1008.7073  937.7873  947.1193 1055.6590\n [64] 1134.0258 1092.5467  877.6170 1067.0919 1165.0152 1097.8645 1125.9920\n [71]  895.4531 1055.2484 1076.5924 1071.7679 1209.6463 1125.4865  968.7989\n [78] 1138.3152 1154.0843  936.4563  940.2080 1065.3797  884.6532  843.2516\n [85] 1000.2088  955.3850 1047.8846 1170.0051 1175.3559 1162.4729 1005.8265\n [92] 1013.7478  919.3033 1051.0724  942.7679 1178.7499  978.6767 1118.7880\n [99]  949.3146 1003.0768  967.5269 1165.6341 1016.6392 1100.9496 1106.7751\n[106]  873.7377 1045.2197 1187.2821 1062.3207  936.8004  866.6742  921.0374\n[113] 1111.3470 1019.4120  997.5843  878.4902  949.6300 1018.3885 1050.6530\n[120]  891.1566 1050.1545  969.3831 1061.6462  998.0876 1069.8626 1103.7155\n[127] 1075.5635 1083.0892 1080.7179 1025.5224 1066.6942  912.5556 1089.3433\n[134] 1101.9849\n\n\nFinally, you can get the fifth observation from the fifth variable:\n\ndata[5,5]\n\n[1] 986.7462\n\n\nPretty useful, right? Variables usually also have names, and we can use the $ operator to obtain specific variables (or columns) by name rather than by number. For example:\n\ndata$ForewingArea\n\n  [1]  964.8420 1064.9463 1087.6290 1025.4572  986.7462 1029.7973 1092.1279\n  [8] 1030.1300  951.0215  952.0203  972.7687  923.5299  965.2786 1067.0532\n [15]  806.4759  933.6333  984.8791  900.8961  896.5896 1073.4148 1031.5598\n [22] 1122.3702  923.2774 1010.8952 1183.9889 1085.7107 1159.4511 1023.0000\n [29]  861.2154  906.4139 1046.9687 1165.4515 1037.9212  900.6176 1093.4250\n [36] 1050.7947  922.8024  929.6439 1045.5846  832.4090  984.0448 1059.0467\n [43]  886.6116  928.6071  992.8063 1117.8374 1012.8004 1057.8563 1174.0423\n [50] 1078.7093  982.5831 1141.2504 1117.7142 1049.4940  945.7766 1193.0925\n [57]  993.9152  900.3906  977.3681 1008.7073  937.7873  947.1193 1055.6590\n [64] 1134.0258 1092.5467  877.6170 1067.0919 1165.0152 1097.8645 1125.9920\n [71]  895.4531 1055.2484 1076.5924 1071.7679 1209.6463 1125.4865  968.7989\n [78] 1138.3152 1154.0843  936.4563  940.2080 1065.3797  884.6532  843.2516\n [85] 1000.2088  955.3850 1047.8846 1170.0051 1175.3559 1162.4729 1005.8265\n [92] 1013.7478  919.3033 1051.0724  942.7679 1178.7499  978.6767 1118.7880\n [99]  949.3146 1003.0768  967.5269 1165.6341 1016.6392 1100.9496 1106.7751\n[106]  873.7377 1045.2197 1187.2821 1062.3207  936.8004  866.6742  921.0374\n[113] 1111.3470 1019.4120  997.5843  878.4902  949.6300 1018.3885 1050.6530\n[120]  891.1566 1050.1545  969.3831 1061.6462  998.0876 1069.8626 1103.7155\n[127] 1075.5635 1083.0892 1080.7179 1025.5224 1066.6942  912.5556 1089.3433\n[134] 1101.9849\n\n\nAnd if you want the fifth observation for this variable:\n\ndata$ForewingArea[5]\n\n[1] 986.7462\n\n\nNote than in this case [] is one dimensional. That is because data$ForewingArea returns a one dimensional (AKA vector) matrix with the values for this variable.\nAnd if you want to obtain the first four columns of the dataframe you can do the following:\n\ndata[1:5,]\n\n  X Species Status Nparasites ForewingArea\n1 1 Viceroy      1          2     964.8420\n2 2 Monarch      3          0    1064.9463\n3 3 Viceroy      2          2    1087.6290\n4 4 Viceroy      2          3    1025.4572\n5 5 Monarch      2          1     986.7462\n\n\nLook at that last result. The X column doesn’t make sense. However, it is super easy to remove it. we can use [,-1] to remove the first column:\n\ndata&lt;-data[,-1]\ndata\n\n    Species Status Nparasites ForewingArea\n1   Viceroy      1          2     964.8420\n2   Monarch      3          0    1064.9463\n3   Viceroy      2          2    1087.6290\n4   Viceroy      2          3    1025.4572\n5   Monarch      2          1     986.7462\n6   Viceroy      2          2    1029.7973\n7   Monarch      3          1    1092.1279\n8   Viceroy      2          4    1030.1300\n9   Viceroy      2          1     951.0215\n10  Monarch      2          3     952.0203\n11    Queen      1          9     972.7687\n12    Queen      1          2     923.5299\n13    Queen      1          7     965.2786\n14  Monarch      3          0    1067.0532\n15  Monarch      1          8     806.4759\n16  Monarch      1          4     933.6333\n17  Monarch      2          1     984.8791\n18  Viceroy      1          5     900.8961\n19  Viceroy      1          1     896.5896\n20  Monarch      3          1    1073.4148\n21  Monarch      3          2    1031.5598\n22  Monarch      3          0    1122.3702\n23    Queen      1          6     923.2774\n24    Queen      2          4    1010.8952\n25  Viceroy      3          0    1183.9889\n26    Queen      2          2    1085.7107\n27  Viceroy      3          0    1159.4511\n28  Viceroy      2          0    1023.0000\n29    Queen      1          3     861.2154\n30  Viceroy      1          4     906.4139\n31    Queen      2          4    1046.9687\n32    Queen      3          0    1165.4515\n33    Queen      3          0    1037.9212\n34  Monarch      1          6     900.6176\n35  Monarch      3          1    1093.4250\n36  Monarch      3          0    1050.7947\n37  Viceroy      1         10     922.8024\n38  Viceroy      2          1     929.6439\n39  Viceroy      2          4    1045.5846\n40    Queen      1          2     832.4090\n41    Queen      1          3     984.0448\n42  Monarch      2          2    1059.0467\n43  Monarch      1          0     886.6116\n44    Queen      1          9     928.6071\n45    Queen      2          2     992.8063\n46  Viceroy      3          1    1117.8374\n47  Monarch      1          6    1012.8004\n48  Monarch      3          0    1057.8563\n49    Queen      3          0    1174.0423\n50    Queen      3          0    1078.7093\n51    Queen      2          0     982.5831\n52  Monarch      3          0    1141.2504\n53  Viceroy      3          2    1117.7142\n54  Viceroy      2          0    1049.4940\n55  Viceroy      1          3     945.7766\n56    Queen      3          0    1193.0925\n57  Viceroy      2          2     993.9152\n58    Queen      1          5     900.3906\n59    Queen      2          2     977.3681\n60    Queen      2          3    1008.7073\n61  Monarch      1          5     937.7873\n62  Viceroy      1          9     947.1193\n63  Viceroy      3          0    1055.6590\n64  Viceroy      3          1    1134.0258\n65  Viceroy      3          0    1092.5467\n66  Monarch      1          2     877.6170\n67    Queen      3          0    1067.0919\n68    Queen      3          2    1165.0152\n69  Monarch      3          0    1097.8645\n70    Queen      3          0    1125.9920\n71  Viceroy      1          3     895.4531\n72  Monarch      2          5    1055.2484\n73    Queen      3          2    1076.5924\n74    Queen      3          0    1071.7679\n75    Queen      3          0    1209.6463\n76  Monarch      3          2    1125.4865\n77  Viceroy      2          4     968.7989\n78    Queen      3          0    1138.3152\n79  Monarch      3          0    1154.0843\n80  Viceroy      1          3     936.4563\n81  Viceroy      1          1     940.2080\n82    Queen      2          7    1065.3797\n83  Viceroy      1          4     884.6532\n84  Viceroy      1          4     843.2516\n85  Viceroy      2          2    1000.2088\n86  Viceroy      1          0     955.3850\n87    Queen      2          3    1047.8846\n88  Monarch      3          1    1170.0051\n89  Monarch      3          0    1175.3559\n90    Queen      3          0    1162.4729\n91  Viceroy      2          4    1005.8265\n92  Monarch      2          4    1013.7478\n93    Queen      1          8     919.3033\n94    Queen      2          1    1051.0724\n95  Viceroy      2          3     942.7679\n96    Queen      3          1    1178.7499\n97  Monarch      2          2     978.6767\n98  Monarch      3          1    1118.7880\n99    Queen      2          0     949.3146\n100 Viceroy      2          2    1003.0768\n101 Viceroy      1          6     967.5269\n102   Queen      3          0    1165.6341\n103   Queen      2          2    1016.6392\n104 Monarch      3          1    1100.9496\n105 Monarch      2          0    1106.7751\n106   Queen      2          0     873.7377\n107 Monarch      3          1    1045.2197\n108   Queen      3          0    1187.2821\n109 Viceroy      2          1    1062.3207\n110 Viceroy      1          5     936.8004\n111 Viceroy      1          3     866.6742\n112 Viceroy      2          4     921.0374\n113   Queen      3          1    1111.3470\n114 Viceroy      2          3    1019.4120\n115   Queen      2          4     997.5843\n116 Monarch      1          5     878.4902\n117   Queen      1          3     949.6300\n118   Queen      2          2    1018.3885\n119 Monarch      2          2    1050.6530\n120 Monarch      1          9     891.1566\n121 Monarch      2          1    1050.1545\n122   Queen      2          1     969.3831\n123 Monarch      2          0    1061.6462\n124   Queen      2          1     998.0876\n125 Monarch      3          0    1069.8626\n126   Queen      3          0    1103.7155\n127 Monarch      3          0    1075.5635\n128 Viceroy      2          3    1083.0892\n129   Queen      2          1    1080.7179\n130 Viceroy      2          1    1025.5224\n131   Queen      2          0    1066.6942\n132 Monarch      1          7     912.5556\n133   Queen      3          1    1089.3433\n134 Viceroy      3          1    1101.9849\n\n\nAnd now it’s gone! Be very careful! Just run that line once. If you were to run it again you would remove the new “first” column (Species).\n\n\n\n\n\n\nNote\n\n\n\nWhile this worked great, the tidyverse (https://www.tidyverse.org/) is better for data transformation and cleaning than baseR (in my opinion). We will work on that later, but the R for Data Science book is available online for free: https://r4ds.hadley.nz/data-visualize I do recommend trying to master base R before moving into tidy or data.tables.\n\n\nFinally, we can also use indices to obtain certain data that we are interested in. We can use logical operators for this. For example:\n\ndataMonarch&lt;-data[data$Species==\"Monarch\",]\nhead(dataMonarch)\n\n   Species Status Nparasites ForewingArea\n2  Monarch      3          0    1064.9463\n5  Monarch      2          1     986.7462\n7  Monarch      3          1    1092.1279\n10 Monarch      2          3     952.0203\n14 Monarch      3          0    1067.0532\n15 Monarch      1          8     806.4759\n\n\nThis gives us a new dataframe that only contains monarchs.\nThese are called “logical operators”. Here are the ones I use the most often:\n\n\n\nOperator\nDescription\n\n\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal to\n\n\n&gt;\nmore than\n\n\n&gt;=\nmore than or equal to\n\n\n==\nequal to\n\n\n!=\ndifferent than\n\n\n!x\nnot x\n\n\nx & y\nx AND y\n\n\nisTRUE(x)\ntest if X is true\n\n\nX%in%Y\nis X in Y?\n\n\n\nAnother cool thing about indices, is that we can use functions on specific columns (or variables) of a dataframe. For example:\n\nmean(dataMonarch$ForewingArea)\n\n[1] 1030.861\n\n\nGives us the mean of the Forewing Area for Monarchs.\nFinally, we can also add columns using the $ operator. The forewing area is in squared millimeters, if we wanted to calculate it in squared centimeters we could do the following:\n\ndataMonarch$FAcm&lt;-dataMonarch$ForewingArea*0.01\nhead(dataMonarch)\n\n   Species Status Nparasites ForewingArea      FAcm\n2  Monarch      3          0    1064.9463 10.649463\n5  Monarch      2          1     986.7462  9.867462\n7  Monarch      3          1    1092.1279 10.921279\n10 Monarch      2          3     952.0203  9.520203\n14 Monarch      3          0    1067.0532 10.670532\n15 Monarch      1          8     806.4759  8.064759",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R, Projects, and Quarto</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#projects",
    "href": "Assignment1.html#projects",
    "title": "1  Intro to R, Projects, and Quarto",
    "section": "1.3 Projects",
    "text": "1.3 Projects\nI really recommend you create a project for each specific research project you have. Common workflows have tons of problems, the main one is that R uses a global workspace. Without projects you are running and working on different analyses in the same global environment. As your code gets more complex, this will get more and more dangerous.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R, Projects, and Quarto</span>"
    ]
  },
  {
    "objectID": "Assignment1.html#quarto",
    "href": "Assignment1.html#quarto",
    "title": "1  Intro to R, Projects, and Quarto",
    "section": "1.4 Quarto",
    "text": "1.4 Quarto\nYou may or may not have heard of RMarkdown or Markdown. For your assignments, you will be using a similar program called Quarto.\nQuarto is a multi-language, next-generation version of R Markdown from Posit. Just like Markdown and RMarkdown, Quarto is a plain text file editor. It has tons of advantages:\n\nIt allows to annotate your code\nYou can embed, run code and show plots.\nThis whole document was written using Quarto, by the end of the semester, I hope you are able to write full reports on Quarto\nYou can mix it with Github and have version control\nCreate a yearly report. You can write the report, and have sections that are data-dependent. As new data comes in, the report auto-updates.\n\nIf you want to learn about how to format in Quarto, you can use the following cheat sheet for RMarkdown (it uses the same syntax): https://posit.co/blog/the-r-markdown-cheat-sheet/\nFor me, the most important aspect of Quarto is embedding code (I call these “call chunks”). To do this you use the following symbol: three times. This way:\n```{r}\nThe ` symbol is on the top left section of your keyboard. Next to the 1, and under the ~.\nYou can also go to insert &gt; executable cell &gt; R.\nWhile I recommend you use scripts in your own research, I will have you use Quarto for your assignments in this class. This will print a “report” with all your answers, code and results, which will be easier for me to grade.\nTo create a new quarto file, go to File &gt; New File &gt; Quarto Document &gt; Write a title and your name &gt; Create.\nMake sure the header has the following information, with the following header (this will make it pretty, and upload all the information):\ntitle: \"Title of document\"\nauthor: \"Your name\"\nformat: \n  html:\n    self-contained: true\neditor: visual\nYou can copy and paste it.\n\n\n\n\n\n\n✏️ Question 1 ✏️ 5pts\n\n\n\n\nCreate a new Quarto file (html). This is the file that you will upload to Canvas.\nWrite an explanation of what you think a statistical model is\nInside the Quarto file, using a code chunk, load the dataset, and then remove the first column X as we did earlier.\nLook at the whole dataset\nIf you did your plot in R, try to embed it using code. If you used a different software option (or drew it), upload it to canvas as an independent file.\n\n\n\n\n\n\n\n\n\n✏️ Question 2 ✏️ 5pts\n\n\n\n\nIn the same file, create an object with only monarchs, one object with only viceroy, and one with only queen.\nEstimate the mean forewing area for each group. Do you think this differences are significant? Can we tell if the differences are because they are different species or because of some other variable (e.g., parasites)\nEstimate the standard deviation of the forewing area for each group\nCreate a new object in which you only have observations that have a forewing area lower than 1000, AND 2 or more parasites.\n\n\n\n\n\n\n\n\n\n✏️ Question 3 ✏️5pts\n\n\n\nYou came up with a new population butterfly condition index (BCI). The equation is fairly simple:\n\\[ BCI_i = \\frac{x_i}{\\overline{x}} \\]\nwhere \\(BCI_i\\) is the index for individual i, \\(x_i\\) is the forewing area for individual i, and \\(\\overline{x}\\) is the sample mean. You need to estimate this for each individual, and you have to use a different \\(\\overline{x}\\) for each species (each species has its own mean forewing area).\nWrite the code needed to estimate this BCI for each individual.\ntip: In the three objects you created (one for each species) add a column that estimates this BCI. Report he maximum and the minimum BCI (functions max and min) for each species.\n\n\nClick Render, and upload the resulting html file to CANVAS. If you can’t get it to work contact me or talk to me after class.\nFinally, please let me know if this assignment and these types of exercises are useful! :)\n\n\n\n\nChilds, Dylan. 2024. “Chapter 10 Working Directories and Data Files.” In. https://dzchilds.github.io/eda-for-bio/working-directories-and-data-files.html.\n\n\nWard, Caitlin, and Collin Nolte. 2024. An Intuitive, Interactive, Introduction to Biostatistics. https://collinn.github.io/sbi/index.html#acknowledgements.",
    "crumbs": [
      "Home",
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to R, Projects, and Quarto</span>"
    ]
  },
  {
    "objectID": "Assignment1Q.html",
    "href": "Assignment1Q.html",
    "title": "Assignment 1",
    "section": "",
    "text": "To create a new quarto file, go to File &gt; New File &gt; Quarto Document &gt; Write a title and your name &gt; Create.\nMake sure the header has the following information, with the following header (this will make it pretty, and upload all the information):\ntitle: \"Title of document\" \nauthor: \"Your name\" \nformat:\n  html:\n    embed-resources: true\neditor: visual\nYou can copy and paste it.\n\n\n\n\n\n\n✏️ Question 1 ✏️ 5pts\n\n\n\n\nCreate a new Quarto file (html). This is the file that you will upload to Canvas.\nWrite an explanation of what you think a statistical model is\nInside the Quarto file, using a code chunk, load the dataset, and then remove the first column X as we did earlier.\nLook at the whole dataset\nIf you did your plot in R, try to embed it using code. If you used a different software option (or drew it), upload it to canvas as an independent file.\n\n\n\n\n\n\n\n\n\n✏️ Question 2 ✏️ 5pts\n\n\n\n\nIn the same file, create an object with only monarchs, one object with only viceroy, and one with only queen.\nEstimate the mean forewing area for each group. Do you think this differences are significant? Can we tell if the differences are because they are different species or because of some other variable (e.g., parasites)\nEstimate the standard deviation of the forewing area for each group\nCreate a new object in which you only have observations that have a forewing area lower than 1000, AND 2 or more parasites.\n\n\n\n\n\n\n\n\n\n✏️ Question 3 ✏️5pts\n\n\n\nYou came up with a new population butterfly condition index (BCI). The equation is fairly simple:\n\\[ BCI_i = \\frac{x_i}{\\overline{x}} \\]\nwhere \\(BCI_i\\) is the index for individual i, \\(x_i\\) is the forewing area for individual i, and \\(\\overline{x}\\) is the sample mean. You need to estimate this for each individual, and you have to use a different \\(\\overline{x}\\) for each species (each species has its own mean forewing area).\nWrite the code needed to estimate this BCI for each individual.\ntip: In the three objects you created (one for each species) add a column that estimates this BCI. Report he maximum and the minimum BCI (functions max and min) for each species.\n\n\nClick Render, and upload the resulting html file to CANVAS. If you can’t get it to work contact me or talk to me after class.\nFinally, please let me know if this assignment and these types of exercises are useful! :)",
    "crumbs": [
      "Home",
      "Introduction",
      "Assignment 1"
    ]
  },
  {
    "objectID": "Readings1.html",
    "href": "Readings1.html",
    "title": "Readings 1",
    "section": "",
    "text": "Please Read Chapter 4 and 5 of (Ward and Nolte 2024).\n\n\n\n\nWard, Caitlin, and Collin Nolte. 2024. An Intuitive, Interactive, Introduction to Biostatistics. https://collinn.github.io/sbi/index.html#acknowledgements.",
    "crumbs": [
      "Home",
      "Introduction",
      "Readings 1"
    ]
  },
  {
    "objectID": "assignment2_intro.html",
    "href": "assignment2_intro.html",
    "title": "The Central Limit Theorem and confidence intervals",
    "section": "",
    "text": "In this section we are going to learn the following:\n\nPlotting and using ggplot. I commonly use ggplot, although still use “baseR” quite often to plot. But the reality is that the ggplot package is probably the best tool and package to plot in R. We will learn to make plots using ggplot and we will use it throughout the course.\nData simulation.\nThe central theorem and confidence intervals\nIntroduction to linear models\n\nThis is a “longer” assignment. But the next one will be shorter. You can take as long as you need on it 😃\nYou can download the quarto document (from Canvas) and use it to answer the questions, copy code, modify code, etc.\n\nRemember!\n\n\n\n\n\n\nNote\n\n\n\nIf you are comfortable with R basics, you can write the code for this exercise directly into Quarto. To add extra code chunks, write this symbols: ```{r}, if you are still not comfortable with R, I recommend for the time being you open a new file and copy and paste the code there, and run it. You can submit this as a qmd file or as an r-script. Whatever you feel more comfortable with at the moment (.qmd + .html preferred)\nYou can also go to insert &gt; executable cell &gt; R and that should insert an r chunk\n\n\n\n\n\n\n\n\nWhy is my data not loading?\n\n\n\nMake sure you download the data file, and also save the quarto file you’re working on in the class folder!",
    "crumbs": [
      "Home",
      "The Central Limit Theorem and confidence intervals"
    ]
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "2  Exploring the CLT",
    "section": "",
    "text": "2.0.0.1 Step 1: Simulate your data\nThe applet on Ward and Nolte’s book (Ward and Nolte 2024) is a great example of the central limit theorem. If you still have questions about this topic I recommend you check the applet on section 6.3 and the one on section 7.2.\nWe are going to do a little experiment regarding the Central Limit Theorem and confidence intervals. We are going to:\nThis may seem like a lot… but it is not! It can be done fairly easily using R. And at this point I will still guide you step by step.\nLet start\nThis may be the most complicated step. We haven’t really talked about distributions, and you’re not expected to know this at this point. If you are not comfortable simulating your own data, I am providing some examples below. Choose the one you like the most, and use it to simulate your data. If you decide to use one of the provided examples, then you can skip this, and go directly to the Section 2.0.0.2 section. If you want to simulate your own population, look at the examples.\nIf you want to simulate the example, let’s do the following:",
    "crumbs": [
      "Home",
      "The Central Limit Theorem and confidence intervals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring the CLT</span>"
    ]
  },
  {
    "objectID": "assignment2.html#supplementary-information.-for-loops-and-a-challenge",
    "href": "assignment2.html#supplementary-information.-for-loops-and-a-challenge",
    "title": "2  Exploring the CLT",
    "section": "4.1 Supplementary information. For loops and a “challenge”",
    "text": "4.1 Supplementary information. For loops and a “challenge”\nThis is not a part of the assignment, but it is a useful guide. There is also a cool challengfe at the end if you want to attempt it.\nFor loops aren’t the only kind of loops. If you remember, the for loops looked like this (see this section on html or a fully rendered document):\n\n\n\nfor loop\n\n\nThere are other types of loops\nFor example, a while-loop is: \nAnd a repeat loop is: \nSome of the important functions to know.\n\nfor iterates over a sequence\nwhile executes a set of statement as long as a logical condition is true\nbreak stops the loop before it loops through all items\nnext skip to next iteration before terminating the code chunk\nif...else might also be good to know to set logical conditions\n\n\n4.1.0.1 Challenge\nYou are going to flip a fair coin 10,000,000 times. However, if you get lucky and get 20 straight tails, you get to stop flipping a coin (and you get 10 extra credit points!) You need the record in which throw you got the 20th straight tail, and print it. If you are able to do all of this, you get the credit!\n\n\n\n\nWard, Caitlin, and Collin Nolte. 2024. An Intuitive, Interactive, Introduction to Biostatistics. https://collinn.github.io/sbi/index.html#acknowledgements.",
    "crumbs": [
      "Home",
      "The Central Limit Theorem and confidence intervals",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Exploring the CLT</span>"
    ]
  },
  {
    "objectID": "Readings2.html",
    "href": "Readings2.html",
    "title": "Supplementary Readings",
    "section": "",
    "text": "Please Read Chapter 6 of (Ward and Nolte 2024).\n\n\n\n\nWard, Caitlin, and Collin Nolte. 2024. An Intuitive, Interactive, Introduction to Biostatistics. https://collinn.github.io/sbi/index.html#acknowledgements.",
    "crumbs": [
      "Home",
      "The Central Limit Theorem and confidence intervals",
      "Supplementary Readings"
    ]
  },
  {
    "objectID": "linearmodels.html",
    "href": "linearmodels.html",
    "title": "Introduction to Models",
    "section": "",
    "text": "What is a model?\nA model is a simplification (or abstraction) of reality that helps us describe, understand or predict a system. Statistical models, help us describe relationships among variables. Which allows us to describe systems, and more specifically, it allows us to do:\nBecause biological systems have uncertainty, we need models in order to answer hypotheses.",
    "crumbs": [
      "Home",
      "Introduction to Models"
    ]
  },
  {
    "objectID": "linearmodels.html#what-is-a-model",
    "href": "linearmodels.html#what-is-a-model",
    "title": "Introduction to Models",
    "section": "",
    "text": "Inference\nPrediction\nExploration",
    "crumbs": [
      "Home",
      "Introduction to Models"
    ]
  },
  {
    "objectID": "linearmodels.html#the-lego-models",
    "href": "linearmodels.html#the-lego-models",
    "title": "Introduction to Models",
    "section": "The Lego models",
    "text": "The Lego models\n\nNotre-Dame\nTake for example, the following lego model:\n\n\n\nNotre Dame model.\n\n\nThat Lego model was obtained from https://www.nytimes.com/2024/06/01/world/europe/lego-notre-dame-cathedral.html. It clearly represents reality. But it obviously is NOT the real Notre-Dame. Most importantly, some things are obviously wrong. Some details are missing, the trees look different, the color is off, the aging doesnt show, it is missing windows, and bells, and many other things.\nHowever, if you showed this to someone that has never seen the real Notre-Dame Cathedral, they would have a very good idea of what it looks like.\nIn biology and other sciences we rarely have access to the real buildings. We only have access to the Lego models. So, we derive our understanding of the natural world, from a series of Lego models that represent different hypothesis. Some are very complex and intricate, some are very simple, some are very hard to understand. Some try to show you the shape of a building, others the usefulness, others, the colors, and others how future real buildings might look like. But most importantly, they are all wrong. One way or another.\nBecause we do not have access to the real world buildings, and we only have access to the Lego models, it is important to know that 1- all models are wrong, 2- some of them are useful, and 3- it is important to know why they are wrong. This is the basis for George Box’s quote:\n\n\n\n\n\n\nGeorge Box and Norman Drapper said\n\n\n\n“Remember that all models are wrong: the practical questions is how wrong do they have to be to not be useful” (Box and Draper 1987)\nRemember, all of your models (ANOVAS, Linear Models, etc) will be wrong! But they can still be useful!\n\n\n\n\n\n\n\n\nThink about it 🧠\n\n\n\nYou have an activity, in which a group of people are trying to recreate a house (think, a typical suburban American house).\nA group of 20 people are given a bag with 30 simple pieces of Lego and are asked to build a typical suburban American house. They have to use all pieces.\nA group of 20 people are given a bag with 20,000 pieces of Lego. Some are very unique pieces. They are also asked to build a typical suburban American house.\nThink about the following:\n1) Which of the two groups will have higher bias (difference between reality and model)?\n2) Which will have higher variance (differences among models)?\n\n\n\n\n\n\nBox, George E. P., and Norman Richard Draper. 1987. Empirical model-building and response surfaces. 7. Dr. Wiley series in probability and mathematical statistics. Applied probability and statistics. New York: Wiley.",
    "crumbs": [
      "Home",
      "Introduction to Models"
    ]
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "3  Linear Models",
    "section": "",
    "text": "3.0.1 Assignment 3\nSo far we have run a categorical linear model with no covariates (also known as a one-sample t-test):\n\\[\n\\begin{split}y_i & \\sim \\beta_0 + \\epsilon_i \\\\\n\\text{where } \\epsilon & \\sim normal(0,\\sigma)\n\\end{split}\n\\]\nThat is the first model you ran in the last assignment (assignment 2).\nThat equation reads as the ith observation of the response variable (y) is given by the intercept \\(\\beta_0\\) plus some “observation” error that is normally distributed.\nYou also ran a couple of 2 sample test (AKA 2 sample t-test) which have the following structure:\n\\[\n\\begin{split}\ny_i & \\sim \\beta_0 + \\beta_1x_i + \\epsilon_i \\\\\n\\text{where } \\epsilon & \\sim normal(0,\\sigma)\n\\end{split}\n\\]\nThat reads as the ith observation of the response variable (y) is given by the intercept \\(\\beta_0\\) plus a coefficient \\((\\beta_1)\\) multiplied by an explanatory variable, plus some “observation” error that is normally distributed.\nLet’s actually look at the tuna example that you should have run.\n\\[\n\\begin{split}\nGrowth_i & \\sim \\beta_0 + \\beta_1(\\text{status infected}) + \\epsilon_i \\\\\n\\text{where } \\epsilon & \\sim normal(0,\\sigma)\n\\end{split}\n\\]\nWhere status (infected) is treated as:\nSo, for healthy individuals, we can obtain the predicted value as:\n\\[\n\\begin{split}\ny_i & \\sim \\beta_0 + \\epsilon_i \\\\\n\\text{where } \\epsilon & \\sim normal(0,\\sigma)\n\\end{split}\n\\]\nWhile for the infected individuals, the predicted value is obtained using the following:\n\\[\n\\begin{split}\ny_i & \\sim \\beta_0 + \\beta_1+ \\epsilon_i \\\\\n\\text{where } \\epsilon & \\sim normal(0,\\sigma)\n\\end{split}\n\\]\nSo, when we run the model in R (you did this last week!) We get the following:\nSo, the mean value for healthy individuals (or expected value for a healty individuals is \\(\\beta_0\\) or 390.16 , while the mean value (or expected value) for an infected individual is \\(\\beta_0 + \\beta_1\\) or 390.16 - 51.76 = 338.4.\nOther good information about this test can be found in the bottom right corner. It’s a p-value. This is the MODEL p-value. Essentially, the model p-value in R compares two hypotheses:\n\\[\n\\begin{split}\n\\text{Ho:} \\ y_i & \\sim \\beta_0 + \\epsilon_i \\\\\n\\text{Ha:} \\ y_i & \\sim \\beta_0 + \\beta_1+ \\epsilon_i \\\\\n\\end{split}\n\\]\nEssentially, it always compares a “null model” where only an intercept is estimated with whatever model you ran. Later in class we will learn how to run and compare multiple models, and that is a great way to get away from the limitations of p-values and hypothesis testing!\nOnly in this case (a simple regression with only one predictor), the model p-value and the p-value for the coefficient will be the same. Check the results and look at the coefficients. Each coefficient has a P-value. For each coefficient, the hypothesis checked are:\n\\[\n\\begin{split}\n\\text{Ho:} & \\text{ coefficient} = 0 \\\\\n\\text{Ha:} & \\text{ coefficient} \\neq0\n\\end{split}\n\\]\nEssentially, if there is an effect of the coefficient, then the result will be significant, and therefore this model will be better than a null model (no effect model).\nAs we move to more complex models, we will get multiple coefficient p-values, and an overall model p-value.\nFinally, there is something I want you to think about:\nIn this assignment we will look at a model with more than two groups in a categorical variable, ANOVA, at a model with one continuous predictor, and at models with both categorical and continuous predictors (ANCOVA). Some of these topics we haven’t seen in class yet.\nYou have two weeks to turn this assignment in, as there will be no assignment next week! :)\nWe will also look at the always scary subject of model assumptions.",
    "crumbs": [
      "Home",
      "Introduction to Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "assignment3.html#model-with-a-categorical-variable-with-more-than-2-groups",
    "href": "assignment3.html#model-with-a-categorical-variable-with-more-than-2-groups",
    "title": "3  Linear Models",
    "section": "3.1 Model with a categorical variable with more than 2 groups",
    "text": "3.1 Model with a categorical variable with more than 2 groups\nLet’s actually work on the data and run models.\nYou are welcome to check for assumptions for every model, but if your head is spinning after reading the assumptions section, then you can skip it and not run assumptions.\nTeporingos 🐰 part two: Remember the last teporingos dataset you worked with last week? Well, turns out that there were a total of four sites (or populations) sampled! However, they were saved in different files!\nSo, you have to load two datasets: teporingos2pops.csv and teporingosnew.csv and somehow fuse them. We will do it using basic r. In a couple weeks you will be introduced to a different way to manage data: the tidyverse.\nRead both datasets:\n\npops1&lt;-read.csv(\"data/teporingos2pops.csv\")\npops2&lt;-read.csv(\"data/teporingosnew.csv\")\n\nAnd let’s combine them:\n\nallpops&lt;-rbind(pops1,pops2)\nhead(allpops)\n\n     site     mass\n1 texcoco 400.3696\n2 texcoco 407.5110\n3 texcoco 413.9215\n4 texcoco 412.7282\n5 texcoco 424.2921\n6 texcoco 418.2353\n\n\nThis code works because both files had the same structure, so, we just bind the rows.\nNow, let’s run the model:\n\nmodelt&lt;-lm(mass~site,data=allpops)\nsummary(modelt)\n\n\nCall:\nlm(formula = mass ~ site, data = allpops)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.180 -10.463  -1.370   7.119  49.499 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   448.857      2.811 159.702  &lt; 2e-16 ***\nsitetexcoco   -23.023      4.216  -5.461 3.85e-07 ***\nsiteTlaloc      2.093      4.216   0.496 0.620713    \nsitetopilejo  -15.184      4.444  -3.417 0.000938 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.39 on 94 degrees of freedom\nMultiple R-squared:  0.3267,    Adjusted R-squared:  0.3052 \nF-statistic:  15.2 on 3 and 94 DF,  p-value: 3.837e-08\n\n\nWow! We ran this model the same way we did the one with 2 groups. That is super useful!\nYou get more coefficients this time. Four of them. Let’s explore the model equation (next week I will ask you to provide the model equations):\n\\[\n\\begin{split}\ny_i & \\sim \\beta_0 + \\beta_1 x_{1,i}+ \\beta_2 x_{2,i} + \\beta_3 x_{3,i}+ \\epsilon_i \\\\\n\\text{where } \\epsilon & \\sim normal(0,\\sigma)\n\\end{split}\n\\]\nThe betas (4 values) are the coefficient estimates. And the x values (there are 3 different values) are:\n\n\n\nSite\nValue of \\(x_1\\)\nValue of \\(x_2\\)\nValue of \\(x_3\\)\n\n\n\n\nPopocatepetl\n0\n0\n0\n\n\nTexcoco\n1\n0\n0\n\n\nTlatelolco\n0\n1\n0\n\n\nTopilejo\n0\n0\n1\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nInterpret the results from the last model, including the p-values of the coefficient and of the model. Calculate the expected mass of an individual from Popocatepetl and one from Tlatelolco by “hand” (you can use r, but input the specific numbers and write the equation, ask me if you don’t understand the question)\n\n\nAfter we run the model, we need to know whether there is an effect of site on weight. We can use an ANOVA for that. There are many ways to run ANOVAS in R. This is my go to (from package car. First, let’s explore what out hypotheses are:\n\n\\[\n\\begin{split}\n\\text{Ho:} & \\ \\mu_1 = \\mu_2 = \\mu_3 = \\mu_4 \\\\\n\\text{Ha:} & \\ \\text{At least one}\\ \\mu\\ \\text{different}\n\\end{split}\n\\]\nSo, if p&lt;0.05, we can reject the null hypothesis. Let’s run it:\n\nAnova(modelt)\n\nAnova Table (Type II tests)\n\nResponse: mass\n          Sum Sq Df F value    Pr(&gt;F)    \nsite       10809  3  15.203 3.837e-08 ***\nResiduals  22276 94                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThat is cool. It actually tests if the four sites are the same!\n\n\n\n\n\n\nQuestion 4\n\n\n\nInterpret the p-value obtained and make a “statistical decision”\n\n\nSo, we now know that at least one is different. But which one? In this course we will be doing a lot of pairwise comparisons. And we will be using the package emmeans. This is a fantastic package, because it lets you do pairwise comparisons in very complex models. For this model, we use a simple Tukey test. However, we essentially use the same code to use emmeans independently of model complexity:\n\nlibrary(emmeans)\n\nWarning: package 'emmeans' was built under R version 4.3.2\n\nemm.s&lt;-emmeans(modelt, \"site\")\npairs(emm.s)\n\n contrast                estimate   SE df t.ratio p.value\n Popocatepetl - texcoco     23.02 4.22 94   5.461  &lt;.0001\n Popocatepetl - Tlaloc      -2.09 4.22 94  -0.496  0.9597\n Popocatepetl - topilejo    15.18 4.44 94   3.417  0.0051\n texcoco - Tlaloc          -25.12 4.44 94  -5.652  &lt;.0001\n texcoco - topilejo         -7.84 4.66 94  -1.682  0.3389\n Tlaloc - topilejo          17.28 4.66 94   3.707  0.0020\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\n\nplot(emm.s, comparisons = T)\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nggplot(data = allpops, aes(x = site, y = mass)) + \n  geom_boxplot(fill=gray(0.7,0.25),color=\"black\",width=0.1) +\n  geom_violin(fill=gray(0.9,0.1),color=\"black\")+\n theme_bw() + theme(panel.border = element_blank(), panel.grid.major = element_blank(),\n                     panel.grid.minor = element_blank(), axis.line = element_line(colour = \"black\"))\n\n\n\n\n\n\n\n\n\nemmeans(modelt,\"site\")\n\n site         emmean   SE df lower.CL upper.CL\n Popocatepetl    449 2.81 94      443      454\n texcoco         426 3.14 94      420      432\n Tlaloc          451 3.14 94      445      457\n topilejo        434 3.44 94      427      441\n\nConfidence level used: 0.95 \n\n\nThis test compares all of the sites.\n\n\n\n\n\n\nQuestion 5\n\n\n\nWhich sites are different?",
    "crumbs": [
      "Home",
      "Introduction to Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "assignment3.html#model-with-a-continuos-variable",
    "href": "assignment3.html#model-with-a-continuos-variable",
    "title": "3  Linear Models",
    "section": "3.2 Model with a continuos variable",
    "text": "3.2 Model with a continuos variable\nWe will now run a model with a single continuous variable. Essentially testing whether the explanatory variable has an effect on the response variable.\nFirst, let’s run read the dataset:\n\nfoodav&lt;-read.csv(\"data/foodav.csv\")\nhead(foodav)\n\n  site Foodavailability ReproductiveEffort\n1    1         3.274107           4.664454\n2    2         4.674298           6.572521\n3    3         5.907927           7.804491\n4    4         3.109825           4.624609\n5    5         2.120704           3.660976\n6    6         0.984446           2.189319\n\n\nThis dataset cotains data on 63 sampled sites of Poeciliopsis baenschi 🐟. This is a small viviparous fish, and you are studying whether the food availability at each site has an effect on the mean reproductive effort (measured as wet weight in grams) of this species.\nThe way we run this model is the same as before:\n\nmodelfish&lt;-lm(ReproductiveEffort~Foodavailability,data = foodav)\nsummary(modelfish)\n\n\nCall:\nlm(formula = ReproductiveEffort ~ Foodavailability, data = foodav)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37653 -0.25549 -0.04903  0.23662  0.45750 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       0.89490    0.07262   12.32   &lt;2e-16 ***\nFoodavailability  1.19387    0.01590   75.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2652 on 61 degrees of freedom\nMultiple R-squared:  0.9893,    Adjusted R-squared:  0.9891 \nF-statistic:  5636 on 1 and 61 DF,  p-value: &lt; 2.2e-16\n\n\nAnd the output looks really similar!\n\n\n\n\n\n\nQuestion 6\n\n\n\nLook at the output and answer:\nIs there an effect of food availability on reproductive effort? What are your statistical hypotheses and what is your conclusion?\n\n\nBecause our variable is continuous, x takes different values. Let’s remember our equation for this model:\n\\[\n\\begin{split}\ny_i & \\sim \\beta_0 + \\beta_1x_i + \\epsilon_i \\\\\n\\text{where } \\epsilon & \\sim normal(0,\\sigma)\n\\end{split}\n\\]Or, in this specific case:\n\\[\nReproductive \\ effort_i \\sim \\beta_0 + \\beta_1Food \\ availability_i + \\epsilon_i \\\\\n\\]\nSo, for a site with a food availability index of one, our estimated reproductive effort would be:\n\n0.89490 + 1.19387*1\n\n[1] 2.08877\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nWhat is the food availability of a site with a food index of 3.87?\n\n\nYou can estimate the food availability and a confidence interval (crit.value * std. error) for an infinity of values. However, the predict.lm function does it for you:\n\npredictedv&lt;-predict.lm(modelfish,foodav,interval=\"co\")\nfoodav&lt;-cbind(foodav,predictedv)\n\nAnd that can be super useful for plotting!:\n\nlibrary(ggplot2)\n\n\nggplot(foodav, aes(x = Foodavailability, y = ReproductiveEffort,ymin=lwr,ymax=upr)) +\n   geom_point() +\n   geom_line(aes(y=fit),color=\"blue\") +\n   geom_ribbon(alpha=0.2)+\n   theme_classic()\n\n\n\n\n\n\n\n\nPretty cool! And kind of easy, right?",
    "crumbs": [
      "Home",
      "Introduction to Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "assignment3.html#model-with-categorical-and-continuous-variables",
    "href": "assignment3.html#model-with-categorical-and-continuous-variables",
    "title": "3  Linear Models",
    "section": "3.3 Model with categorical and continuous variables",
    "text": "3.3 Model with categorical and continuous variables\nIn this case, you are working for a food company that is developing a drug that lowers sugar consumption in rats.\nYou are testing 3 doses: control, mid, and high. You do this in rats that are being fed ad-libitum. You also record the daily consumption of food “pre-trial”, and finally the daily consumption of food “post trial”.\nRead the drugZ csv:\n\ndrugrat&lt;-read.csv(\"data/drug_rat.csv\")\n\nWhere FC is final consumption, IC is initial consumption.\nand run the following model:\n\nmodeld&lt;-lm(FC~IC+Dose,data=drugrat)\nsummary(modeld)\n\n\nCall:\nlm(formula = FC ~ IC + Dose, data = drugrat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2610 -0.6360  0.0000  0.6514  2.2876 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.51804    0.38892   1.332   0.1849    \nIC           0.93384    0.05647  16.537   &lt;2e-16 ***\nDosedose1   -0.44007    0.20011  -2.199   0.0294 *  \nDosedose2   -2.09915    0.20162 -10.412   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9893 on 146 degrees of freedom\nMultiple R-squared:  0.7636,    Adjusted R-squared:  0.7588 \nF-statistic: 157.2 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nQuestions 8-12\n\n\n\n\nWrite the model equation (using betas). You can ask me how to write equations in Quarto!\nInterpret the model output\nRun an Anova (and a Tukey test using emmeans if necessary) to test whether there is an effect of dose on final consumption. Make sure to state the hypotheses!\nPlot the points, and predicted values (it is OK if you struggle with this, we will work on this together on Friday)",
    "crumbs": [
      "Home",
      "Introduction to Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Linear Models</span>"
    ]
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "4  Model Selection and multi-model inference",
    "section": "",
    "text": "4.1 Complexity of Linear Models\nSo far we have only explored two types of linear models (we are currently starting to talk about Generalized Linear Models:\nAs we progress in the class we will look at more complex (both theoretically, and coding-wise) models. We will look into Generalized Linear Models (GLM), mixed-effects models, generalized additive models, and multivariate models. Time (and interest) dependent we might go into neural networks, machine learning and AI.\nIn order to understand those models, you need to understand linear models.\nAlso, so far we have been running a single model for each dataset. In today’s activity you will be tasked with running multiple models for a single dataset. And then comparing models using Maximum Likelihood. These will allow us to explore multiple models.\nBefore continuing: while we have been running multiple models, we haven’t had time to stop and think about how r is computing the models. How does r finds a value for each coefficient \\(\\beta\\)? It has been using least squares to find these values. Essentially, it finds values of \\(\\beta\\) that minimizes the sum of squares of the differences between predicted and observed values. Think of it as finding the line that minimizes residuals! While these are very useful methods, they are not the only ones!\nWe will start using some methods that use Likelihood. During Wednesday class we talked a bit about likelihood and probability. At this point you should be able to understand the difference between the two (even if you can’t describe it). If you find yourself using (or interested in using) Likelihood and Maximum Likelihood Methods, do yourself a favor and download the book by Burnham and Anderson: Model Selection and Multimodel Inference. Which is free to download through the University of Tennessee Libraries! (Burnham and Anderson 2010) It is the most useful book for multi-model inference!\nWe will discuss the following paper next week: (Tredennick et al. 2021) available here!\nWhile I am a huge proponent and user of AIC as an inferential method, the authors clearly disagree with me, and they make some very good points!\nLikelihood description. Hopefully this will help with understanding likelihood.\nLikelihood theory is a paradigm underlying both frequentist and Bayesian statistics, and you need to understand it if you ever want to go deeper into data science and data analyses. This is similar to the example I talked about in class.\nThe theory underlying likelihood deals with a probabilistic model given the parameters \\((\\theta)\\). Remember, the parameters are “the real” values that we are trying to estimate! And in this case I will describe our observations as y. In probability we usually do the following: \\(P(y|\\theta)\\). Essentially, what is the probability of of observing the outcomes represented by y, given \\(\\theta\\).\nIn a coin-toss example, \\(\\theta\\) is the probability of a success (in this case, a success is considered a tail). And we can calculate the probability of observing three straight tails (think of this as \\(y_1 =1, y_2 =1, y_3 =1)\\). In this case we know \\(\\theta\\). It is 0.5. So, the probability of observing three straight tails is 0.125. We can also estimate the probability of observing 1,10 or 1,000,000 straight tails. Or 2 tails and three heads, or any combination of results. Because we know \\(\\theta\\).\nHowever, in the real world, we do not know \\(\\theta\\). That is what we are trying to estimate. Think of \\(\\theta\\) not as a coin-toss now, but as ANY of the following examples: 1) probability that a cow will be pregnant given that it was given a specific drug, 2) probability of survival for a turkey during a season, 3) probability that a food product will spoil after 48 hours at room temperature following a new packing procedure, 4) probability of a fish passing through a fish ladder it approached, 5) probability of a manufacturing defect in a new wood processing method.\nIn these cases, we do not know the probability! That is what we are trying to estimate, however, we do know the outcomes! We sample our population and know the results. Going back to the coin-toss example, assume we don’t know the probability of getting tails in a coin-toss. We don’t know if the coin is fair or not. However, you do an experiment and toss the coin 10 times and record the results. You replicate this experiment 25 times. Now you have 25 observations (\\(y_i\\)), where each y is the number of successes observed in each experiment i. However, we can estimate the probability of observing what we did if \\(\\theta\\) was 0.01, and the probability of observing wat we did if it was 0.02, and if it was 0.03, or 0.04, and so on. This allow us to find a parameter \\(\\theta\\) that would maximize the following function:\n\\[\n\\mathcal{L}(\\theta|y) = P(y|\\theta)\n\\]\nEssentially, maximum likelihood is a method that answers the following: “what value of theta would maximize the probability of observing what you actually observed?” By definition, the Likelihood function is conditional on the observed data, and is a function of the unknown parameter \\(\\theta\\).",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Selection and multi-model inference</span>"
    ]
  },
  {
    "objectID": "assignment4.html#complexity-of-linear-models",
    "href": "assignment4.html#complexity-of-linear-models",
    "title": "4  Model Selection and multi-model inference",
    "section": "",
    "text": "Simple Linear Models\nMultiple Linear models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe will talk more about Likelihood next week. Last Wednesday we discussed the differences between Likelihood and Probability, hopefully that is still partially clear in your mind",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Selection and multi-model inference</span>"
    ]
  },
  {
    "objectID": "assignment4.html#testing-different-models",
    "href": "assignment4.html#testing-different-models",
    "title": "4  Model Selection and multi-model inference",
    "section": "4.2 Testing different models",
    "text": "4.2 Testing different models\nThe beauty of model selection and multi-model inference is that we are not constrained by the limits of hypothesis testing. More often that not, we already know the answer to a hypothesis test. We already know that there are differences among populations, or that there is an effect of x on y. Using multi-model inference or other model selection methods we can test different hypotheses.\nTo do this, we will use different methods. First, we will simply compare the \\(r^2\\) values of each model. We will also use an information criterion, and finally we will use model cross-validation.\nDownload the grasslanddata.csv file. And explore it.\nThis dataset has 6 variables:\nResponse variable:\nKgDMHA: Pasture mass. Measured in kilograms of fry matter per hectare\nAbiotic explanatory variables:\nWater, salinity, and nitrogen\nBiotic variables:\nGraze (average number of grazing animals observed in the area)\nPests: Proportion of grassland where pests are found. Divided in low (low proportion), half (about half of it has pests), and most.\nYou will be running all of the following models (keep reading before you start running models!):\n\nNull model\nModel with only effect of irrigation\nModel with with a quadratic effect of irrigation\nModel with all abiotic effects (additive)\nModel with all abiotic effects (interactive)\nModel with all biotic effects (additive)\nModel with all biotic effects (interactive)\nModel with all effects (additive)\nRun 1 model not described in this list\nModel with all effects (interactive)\n\nThis is called a “global” model. It is your most complex model\nThis is the model we use to test the assumptions\nNo need to plot this model\n\n\nYou will be running all the models except #10. I will help with that one 😃.\n\ngrasslanddata&lt;-read.csv(\"data/grasslanddata.csv\")\n\nThe first model we run is the global model, that takes all 5 variables, and all the potential interactions:\n\\[\n\\begin{split}\ny \\sim \\beta_0+\\beta_1x_1+ \\\\\n\\beta_2x_2+\\beta_3x_3+\\beta_4x_4+\\beta_5x_5+ \\\\\n\\beta_6x_1x_2+\\beta_7x_1x_3+\\beta_8x_2x_3+ \\\\\n\\beta_9x_1x_4+\\beta_{10}x_2x_4+\\beta_{11}x_3x_4+ \\\\\n\\beta_{12}x_1x_5+\\beta_{13}x_2x_5+\\beta_{14}x_3x_5+ \\\\ \\beta_{15}x_4x_5+\\beta_{16}x_1x_2x_3+\\beta_{17}x_1x_2x_4+\\\\ \\beta_{18}x_1x_3x_4+\\beta_{19}x_2x_3x_4+\\beta_{20}x_1x_2x_5+\\\\ \\beta_{21}x_1x_3x_5+\\beta_{22}x_2x_3x_5+\\beta_{23}x_1x_4x_5+\\\\\n\\beta_{24}x_2x_4x_5+\\beta_{25}x_3x_4x_5+\\beta_{26}x_1x_2x_3x_4+\\\\ \\beta_{27}x_1x_2x_3x_5+\\beta_{28}x_1x_2x_4x_5+\\\\ \\beta_{29}x_1x_3x_4x_5+\\beta_{30}x_2x_3x_4x_5+\\beta_{31}x_1x_2x_3x_4x_5  \n\\end{split}                 \n\\] Woah! That is a lot. You only have to write the equations for three models, just be sure to udnerstand where they are coming from! Now, let’s run the model:\n\\[\n\\frac{123}{200}\n\\]\n\nmodel10&lt;-lm(KgDMHA~Water*Salinity*Nitrogen*Pests*Graze,data = grasslanddata)\n#summary(model10)\nAIC(model10)\n\n[1] 14894.11\n\n\nUsing plot(lm) will plot some exploratory plots that allow you to test assumptions. Essentially, plot 1 and 2 are enough to check the 3 main assumptios.\n\nplot(model10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#plot(resid(model10)~grasslanddata$KgDMHA)\n\nWhile some of the plots might not look great, in reality this is how most data looks like. In this case values over 8,000 kgs and under 5,000 kgs are rare, which makes the variance seem smaller in those values. However, it is usually pretty obvious when you need to transform your data and we will work on that later. In this case, we are meeting the assumptions, so we can run linear models without any transformations.\nNow that we ran it, we can run all of the other models, make sure to follow the outlined steps\n\n\n\nCall:\nlm(formula = KgDMHA ~ 1, data = grasslanddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3792.2  -854.7   -17.2   847.3  4500.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5972.2       38.9   153.5   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1230 on 999 degrees of freedom\n\n\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: The `fun.y` argument of `stat_summary()` is deprecated as of ggplot2 3.3.0.\nℹ Please use the `fun` argument instead.\n\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = KgDMHA ~ Water, data = grasslanddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3210.3  -705.0   -15.5   782.0  3821.6 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.020e+03  3.050e+02   3.344 0.000856 ***\nWater       7.094e-03  4.340e-04  16.344  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1093 on 998 degrees of freedom\nMultiple R-squared:  0.2112,    Adjusted R-squared:  0.2104 \nF-statistic: 267.1 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\n\n\n\nModel 6\n\n\n\nCall:\nlm(formula = KgDMHA ~ Graze + Pests, data = grasslanddata)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2556.9  -629.3    22.9   608.7  2854.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 8508.848    105.262   80.83  &lt; 2e-16 ***\nGraze        -57.008      2.446  -23.31  &lt; 2e-16 ***\nPestslow     844.351    112.874    7.48 1.62e-13 ***\nPestsmost   -892.254     62.145  -14.36  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 909 on 996 degrees of freedom\nMultiple R-squared:  0.4557,    Adjusted R-squared:  0.4541 \nF-statistic:   278 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nQuestion 4 📝 10 pts\n\n\n\nFor the models do the following:\n\nWrite the equation (only use y, x, and \\(\\beta's\\)).\n\nOnly do this for models 1, 2, 3, 4, and 6.\nTo write equations, write two dollar signs $ $\nGoogle “Rmarkdown math” for help with how to write equations\nYou can also right click on any equation on my assignment and select show math as Tex command to see how I wrote the equation.\n\nRun the model\nPrint the output\nPlot the model (only models with one effect or one categorical and one continuous effect)\n\nWhat do I mean with plot the model? Plot the data (observed), and the predicted values (the line), as well as confidence intervals)\nYou don’t NEED to plot the models with three or more effects, but I encourage you to try to think or find a way to plot them\n\n\n\n\nSome important coding tips:\n\nAdditive models have a +\nInteractive models have a *\nYou can mix interactive and additive effects. The order is important though!\nRun quadratic models using: poly(x,2) You can run any polynomial term using poly(x,degree). We haven’t talked about this!\n\n\n\n\n\n\n\nBe careful with naming your model!\n\n\n\nGive each model a different name, and make it a name that makes sense! We will reference the models back!",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Selection and multi-model inference</span>"
    ]
  },
  {
    "objectID": "assignment4.html#model-selection",
    "href": "assignment4.html#model-selection",
    "title": "4  Model Selection and multi-model inference",
    "section": "4.3 Model selection",
    "text": "4.3 Model selection\nWe will use three different methods to compare models and select our best model.\n\n\n\n\n\n\nThink about it 🧠\n\n\n\nWe will discuss this on Monday and Wednesday: what makes a model “better”? Start thinking about it\n\n\n\n4.3.1 \\(R^2\\)\nWe haven’t talked about correlation and coefficient of determination\nCorrelation: The amount of linear association between two variables. It’s measured as \\(r\\)\nThe R-squared value, denoted by R2, is the square of the correlation. It measures the proportion of variation explained by the model. Essentially, if our predicted model explained and landed perfectly on each observation, then In each of the models we ran, we can obtain a\nAn \\(R^2\\) value gives us the proportion of the data variance that was explained by the model.\n\n\n\n\n\n\nThink about it 🧠\n\n\n\nWe will discuss this on Monday: is a higher \\(r^2\\) always better? When is it not better?\n\n\n\n\n\n\n\n\nAssignment question 5:\n\n\n\nCheck the outputs of the models you ran, and find the one with the highest \\(R^2\\). Do you think it is the best model? You should use the adjusted \\(R^2\\)\n\n\n\n\n4.3.2 AICc\nWe will talk more about AICc later in class. For the time being what you need to know is that it is an information criterion that uses maximum likelihood. While we can use maximum likelihood to estimate expected values and obtain estimates, we can also use it to compare models. In this case AIC compares models, and the model with the lowest AIC is the best. I attempt an explanation here, but if you don’t understand it, hopefully next classes will make it clearer!\nAIC has the following equation:\n\\[\nAIC = -2log(\\mathcal{L}(\\hat{\\theta}|data))+2K.  \n\\]\nWhich may look complicated. But it is an actually pretty simple equation with two important parts:\n1) The expression \\(log(\\mathcal{L}(\\hat{\\theta}|data)\\) is the numerical value of the log-likelihood at its maximum point (see Likelihood description or Burnham and Anderson). This maximum point on the log-likelihood function corresponds to the values of the maximum likelihood estimates.\nWhat does this mean? A good way of thinking about it is: “What is the likelihood of observing the data you did, if the model was real?” The Higher the Likelihood the “evidence” of a better model (or better fit of the data to the model). If you notice the equation is -2 times the log-likelihood. Essentially meaning: a lower value is a better model.\n2) The expression \\(2K\\) is simply 2 times the number of parameters (K). It is interpreted as a penalty for increasing the number of parameters. It increases parsimony\nEssentially this means, AICc chooses the model with the highest Likelihood, while penalizing higher complexity. There are two reasons for this:\n1) Every time you add a parameter the Likelihood goes up.\n2) While higher complexity tends to decrease bias, it increases variance. The optimum model complexity is found looking at both the Likelihood and the complexity.\n\n\n\n\n\n\nThink about it 🧠\n\n\n\nWhy does adding parameters result in higher Likelihood?\n\n\nBecause AIC has a negative log-likelihood term and a positive K term LOWER VALUES ARE BETTER. The model with the lowest AIC is “the best model”.\nImportantly, the AIC value doesn’t tell us anything by itself. It is purely a comparative value, where lower is better. We need to estimate \\(\\Delta AIC_i\\) which is the difference between the best model and each model i.\n\n\n\n\\(\\Delta AIC_i\\)\nLevel of Empirical Support of Model i\n\n\n\n\n0-2\nSubstantial\n\n\n2-4\nLess\n\n\n4-7\nConsiderably Less\n\n\n&gt;10\nEssentially none\n\n\n\nOk, let’s now use AIC to analyze our models.\nWe are using a corrected version of AIC that is ideal for small samples (I recommend you always use this one):\n\\[\nAIC_c = AIC +\\frac{2K(K+1)}{n-K-1}\n\\]\nFirst save all the models in a list. I have some sample code here, but do it with your models (you already run them!)\n\nCandidateModels&lt;-list(\"null\"=model1,\"model2\"=model2...)\n\nNow, download and load the AICcmodavg package, and run:\n\nselectionTable &lt;- aictab(cand.set = CandidateModels)\n\nAnd that’s it! What model or models are best? This is super easy to run!\n\n\n\n\n\n\nAssignment question 6:\n\n\n\nPresent your selection table, and describe what model was the best.\n\n\nYou probably have also heard of likelihood ratios. This essentially compared the Likelihood of two models. You can run them pretty easily in R. But we will leave that for some other time. I am terrible at estimating how much time my assignments take students, but I think that this lab is already very time consuming!\nIf you are curious about how to run them, check this documentation by the AICcmodavg package: vignette",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Selection and multi-model inference</span>"
    ]
  },
  {
    "objectID": "assignment4.html#cross-validation",
    "href": "assignment4.html#cross-validation",
    "title": "4  Model Selection and multi-model inference",
    "section": "4.4 Cross-Validation",
    "text": "4.4 Cross-Validation\nOftentimes a criticism when using linear models to predict values, is that we are using the same dataset to explore the data, to create model, to make inference, and to predict the results.\nA way to deal with this, is by using cross-validation, in cross-validation we use a portion of the dataset to “train” the data, and a portion to “test” it.\n\n\n\n\n\n\nThink about it 🧠\n\n\n\nWhy is it useful to separate your data-set in two to test your predictions useful?\n\n\nWe will do that for the following three models:\nKgDMHA~ Water + Salinity + Nitrogen \nKgDMHA~ Pests + Graze \nKgDMHA~ Water + Salinity + Nitrogen + Pests + Graze\n\n\n\n\n\n\nAssignment question 7:\n\n\n\nFollow these seven steps:\n1) Randomly split your dataset, with ~75% of the data for the training and ~25% of the data for the test\n2) With the training dataset, run each of the three models. You should obtain an output for each model\n3) Obtain an AICc value for each of the three models.\n\nAIC(model)\n\n4) In the test dataset, add three new columns called predictmodel1, predictmodel2, precitmodel3. Populate the columns with the predicted values based on each of the three models you ran. You can use the coefficients, or the predict function to obtain those values. It’s easiest to use the predict.lm function. Let me know if you need help with this! Or check the last assignment!\n5) Use the following equation:\n\\[\nRMSE = \\sqrt{\\frac{\\sum(y_i-\\hat{y}_i^2)}{n}}   \n\\]\nRMSE is the root mean square error. Essentially, a measure of the distance between predictor and observation.\nEstimate RMSE for EACH of the three models. Lower RMSE is better!\n6) According to the RMSE, what model is the best?\n7) Compare the RMSE with the AICc and with the R-squared from the models. Do all three metrics agree on the best model?\n\n\n\nWhile this is great, it’s painful and costly to separate your data! You want to use 100% of your data to make predictions! Also, oftentimes our datasets aren’t big enough to separate them in two. In order to deal with this, we will use a cross-validation method that requires only one dataset. We will use a particular kind of cross validation known as K-fold. The conceptual steps of K-fold cross validation for model selection are as follows:\n\nRandomly divide the data set into k number of groups (preferably equal size)\nFit a model to all but one of the groups\nCalculate a metric such as RMSE using the observations from the k-th group that was not used to train the model\nRepeat this process k-number of times using each group\nCalculate the overall RMSE as the average of each calculated above\nRepeat this process for each separate model you wish to compare\nCompare the metric to the estimated metric from other possible models to select the ‘best’ model\n\nThe point is, you are testing your model in data that was not used to develop the model. Does that make sense? If not, wait until Monday or raise your hand! :)\nIt is easy to do:\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\nctrl &lt;- trainControl(method= 'cv', number= 10)\n\nabiotic &lt;- train(KgDMHA~ Water + Salinity + Nitrogen, data= grasslanddata, trControl= ctrl,method=\"lm\")\n\nWe can get the RMSE for each model using:\n\nprint(abiotic)\n\nLinear Regression \n\n1000 samples\n   3 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 900, 900, 900, 900, 900, 900, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  958.6632  0.3979449  771.4688\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nAnd we can get the predictive values from the training set using:\n\nsummary(abiotic$finalModel)\n\n\nCall:\nlm(formula = .outcome ~ ., data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2991.99  -652.64    27.78   654.49  3105.87 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.373e+02  3.162e+02   1.383    0.167    \nWater        7.154e-03  3.805e-04  18.802   &lt;2e-16 ***\nSalinity    -1.189e+02  9.003e+00 -13.207   &lt;2e-16 ***\nNitrogen     4.556e+02  3.969e+01  11.479   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 958.3 on 996 degrees of freedom\nMultiple R-squared:  0.395, Adjusted R-squared:  0.3931 \nF-statistic: 216.7 on 3 and 996 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n\n\nAssignment question 8:\n\n\n\nReport the three RMSE values.\nIn future assignments you might end up running multiple models. We will use a combination of tidy, loops and other tools to test how to run these models and compare them relatively quickly.\n\n\n\n\n\n\nBurnham, Kenneth P., and David Ray Anderson. 2010. Model selection and multimodel inference: a practical information-theoretic approach. 2. ed. New York, NY: Springer.\n\n\nTredennick, Andrew T., Giles Hooker, Stephen P. Ellner, and Peter B. Adler. 2021. “A Practical Guide to Selecting Models for Exploration, Inference, and Prediction in Ecology.” Ecology 102 (6): e03336. https://doi.org/10.1002/ecy.3336.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model Selection and multi-model inference</span>"
    ]
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "5  Generalized Linear models",
    "section": "",
    "text": "5.1 This assignment\nJust as a reminder… We use GLM’s when the predictor and response variables DO NOT have an underlying normal distribution of the residuals or lack an underlying linear relationship.\nIn Linear Models we model the response as a function of the predictors.\nIn GLM, we model a function of the response as a function of the predictors. That function is called the link function. So, we are not modeling the response variable (usually “y”) but a link function of the response. So, instead of modeling:\n\\[\n\\hat{y} = \\beta_0+\\beta_1x_1\n\\]\n(where \\(\\hat{y}\\) represents the expected value of y) we model:\n\\[ logit(p) = \\beta_0+\\beta_1x_1 \\]or\n\\[ log(\\lambda) = \\beta_0+\\beta_1x_1 \\]\nWe use logit for logistic regression (aka binomial distribution), and we use log for Poisson distributed data.\nHere is a table with the link functions for both Binomial and Poisson distributions.\nSave it somewhere important!\nWe went through the Poisson example in class. You can also download the presentation file (from Canvas) and access all of the code we used. During this assignment we will focus on the following:",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear models</span>"
    ]
  },
  {
    "objectID": "assignment5.html#this-assignment",
    "href": "assignment5.html#this-assignment",
    "title": "5  Generalized Linear models",
    "section": "",
    "text": "Logistic regression\nQuestions where you are presented with data, and a research question and you have to analyze the data. You need to decide what test or model to use, and interpret the results.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear models</span>"
    ]
  },
  {
    "objectID": "assignment5.html#logistic-regression",
    "href": "assignment5.html#logistic-regression",
    "title": "5  Generalized Linear models",
    "section": "5.2 Logistic Regression",
    "text": "5.2 Logistic Regression\nWe use the logistic regression when we have binomial data. Remember, in binomial data the response variable is binary, our responses are limited to 0’s and 1’s. Which is which depends on you, but usually 1 is seen as a “success” or “positive”.\nSome examples:\n\nPresence/absence\nAlive/Dead\nHomozygous/Heterozygous\nMature/non mature\nMale/Female\npregnant / not pregnant\nHealthy / Disease\n\n\n\n\n\n\n\nThink about it 🧠\n\n\n\nWhat is the response variable? 🤔\nWe usually obtain the mean of the response variable. In the grasslandexample, the response variable was KgDMHA, or Kilograms of Dry Matter per Hectare.\nIn the logistic regression, we are trying to find the probability of an event happening. Look at the examples before this box. If the binary outcomes are pregnant or not pregnant, the response variable is the probability of being pregnant.\nIf we were exploring whether cortisol has an effect on succesful pregnancies in mice, and the model was glm(pregnancy~cortisol, data=data, family=binomial(link=\"logit\") then, what we are trying to estimate are 1) whether cortisol has an effect on the probability of being pregnant (inference) and 2) what is the probability of a mouse being pregnant given a specific cortisol measurement. This explains why it is not linear. Probabilities go from 0 to 1.\n\n\nThe link equation is the “log of odds”, also known as logit. This equation allows us to move from a system were the values go from 0 to 1 (probability) to one where we can theoretically go from -INF to INF.\n\n5.2.0.1 Log of odds? What is that? And what are odds?\nThe logit link function is: \\(\\mu = log(\\frac{p}{1-p})\\) or also known as “the log of odds”. This works because any probability can be converted to log odds by finding the odds and taking the log of that. James Jaccard calls log odds  ”counterintuitive and challenging to interpret”. They are not as easy to interpret as the “log” we use in Poisson. However, I don’t think you absolutely need to understand the transformation, you only need to understand why it is useful!\nFirst off, the odds is \\(\\frac{p}{1-p}\\). It is simply dividing the probability of an event occurring divided by the probability of it not occurring. Let’s look at some examples:\nIn a coin toss with 0.5 probability, the odds ratio is: \\(\\frac{0.5}{1-0.5} = 1\\). We can interpret this as the odds of getting a success (or head) is 1:1 (think 1 to 1, or 50-50)\nLet’s imagine the probability of getting a 5 after rolling one die. The probability of this event is \\(\\frac{1}{6}\\), so, the odds are:\n\n(1/6)/(5/6)\n\n[1] 0.2\n\n\nThe odds are 0.2. This means you will see 0.2 successes for every failure (or one success for every 5 failures).\nFinally, let’s think of the odds of me (Alejandro) seeing a car accident during my daily drive to and from campus). I know this is biased, and one semester I will actually sample this for an exercise, but my very biased estimate says the probability is 0.8 (80% chance of seeing one). If this was true, then the odds would be:\n\n0.8/(1-0.8)\n\n[1] 4\n\n\nFour. Again, think of this as four to one. If I drove five times, I would see an accident 4 times, and “no accidents” one time.\nOK, that was a lot of time spent on odds. And the reason I did that, is that my brain struggles thinking in odds rather than probabilities. Which is why we want to present the results in a probabilistic scale. However, this concept is important to understand how it is estimated.\nOdds are always “positive”. But the transformation need to potentially be from -inf to inf. Similar to what we do with counts, then we take the log of the odds.\nLet’s imagine the probability of winning the lottery is 0.000000000065789. Then, the log odds would be:\n\nlog(6.5789e-11/(1-6.5789e-11))\n\n[1] -23.44457\n\n\n-23.44. And if we had an event with high odds (like the accident one):\n\nlog(0.8/(1-0.8))\n\n[1] 1.386294\n\n\nHere you can see how values higher than 1 (p higher than 0.5) will be positive, and odds lower than 1 (p lower than 0.5) will be negative.\nActually, we can plot the relationship between p (probability) and logit(pi):\n\npi = seq(0.0001,1-0.0001, by=0.0001)\nplot(pi~qlogis(pi), type=\"l\",xlab=\"logit(p)\",ylab=\"p\"); abline(v=0, lty=\"dashed\"); abline(h=0.5, lty=\"dashed\")\n\n\n\n\n\n\n\n\nWhat I want you to take away from this whole section is the following:\n\n\n\n\n\n\nTake home message 🏠\n\n\n\n1- logit(p) ranges from -Inf to +Inf as pi increases from 0 to 1 Take home message\n2- logit(p) takes on a full range of values which allow the modeling algorithm to explore a full range of coefficient values in the systematic component of the model In other words, \\(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_mx_m\\) can be unconstrained and can take values from -Inf to + Inf\n3- logit(p) is qlogis(p) in r\n\n\n\n\n5.2.0.2 Let’s actually do a regression\nThis dataset (parasitecod.csv) was obtained from the following book:\nZuur, A., Ieno, E. N., Walker, N., Saveliev, A. A. & Smith, G. M. Mixed Effects Models and Extensions in Ecology with R. (Springer New York, 2009).\nIt’s a highly recommended book (I showed it to you during the first week!)\nLoad it into R and let’s run a glm.\n\ncod&lt;-read.csv(\"data/parasitecod.csv\")\nlibrary(dplyr)\ncod&lt;-cod%&gt;%mutate(across(c(Year,Sex,Stage,Area),as.factor))\n\nBefore running the glm, I changed some categorical data to “factor” in R. This step is really important, because if you don’t do it, it will take the data as continuous!\nNow, let’s run the model!\n\ncodmodel1&lt;-glm(Prevalence~Length+Year+Area,data=cod,family = binomial(link=\"logit\"))\nsummary(codmodel1)\n\n\nCall:\nglm(formula = Prevalence ~ Length + Year + Area, family = binomial(link = \"logit\"), \n    data = cod)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.465947   0.269333  -1.730 0.083629 .  \nLength       0.009654   0.004468   2.161 0.030705 *  \nYear2000     0.566536   0.169715   3.338 0.000843 ***\nYear2001    -0.680315   0.140175  -4.853 1.21e-06 ***\nArea2       -0.626192   0.186617  -3.355 0.000792 ***\nArea3       -0.510470   0.163396  -3.124 0.001783 ** \nArea4        1.233878   0.184652   6.682 2.35e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1727.8  on 1247  degrees of freedom\nResidual deviance: 1537.6  on 1241  degrees of freedom\n  (6 observations deleted due to missingness)\nAIC: 1551.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nRemember that those coefficients (think of each coefficient as a \\(\\beta\\), so this model has 7 \\(\\beta's\\)) are for the link function:\n\n\n\n\n\n\nAssignment question 1 📝\n\n\n\nLook at the summary. And before continuing make sure you understand it. If you don’t, now is the time to raise your hand.\nYou need to calculate the expected probability (we’ll call this \\(\\pi\\)) of an individual being infected for each of the following two cases:\n\nAn individual of length 50 from Area 1, in 1999\nAn individual of length 50 from Area 3, in 2001\n\nRemember that:\n\\[\nlog(\\frac{\\pi_i}{1-\\pi_i}) = \\beta_0 +  \\beta_1x_{1,i} + \\beta_2x_{2,i} + ... + \\beta_mx_{m,i}\n\\]\nAnd we are trying to solve for \\(\\pi\\)\nPlease do not use the function “predict”. You can use algebra to solve this problem, or you can use the qlogis and plogis() functions.\n\n\nWe can use the packages car and emmeans to run an ANOVA and look for differences among some of the explanatory variables\nFirst we run:\n\nAnova(codmodel1)\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: Prevalence\n       LR Chisq Df Pr(&gt;Chisq)    \nLength    4.713  1    0.02993 *  \nYear     54.421  2  1.523e-12 ***\nArea    143.079  3  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd see that there are significant effects of all three factors.\nLet’s explore the effects of Year running pairwise comparisons using emmeans:\n\nemmeans(codmodel1, ~Year) %&gt;% contrast(\"pairwise\")\n\n contrast            estimate    SE  df z.ratio p.value\n Year1999 - Year2000   -0.567 0.170 Inf  -3.338  0.0024\n Year1999 - Year2001    0.680 0.140 Inf   4.853  &lt;.0001\n Year2000 - Year2001    1.247 0.179 Inf   6.958  &lt;.0001\n\nResults are averaged over the levels of: Area \nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nPlease note that 1) results are averaged over the levels of Area, and 2) results are given on the log odds ratio scale. This is not the response scale. The response scale is probabilistic, so it goes from 0 to 1 (and therefore there could not be a difference of 1.247\n\n\n\n\n\n\nAssignment question 2 📝\n\n\n\nRun a pairwise comparison for Area, and a pairwise comparison for Year and Area\n\n\nFinally, let’s plot it!\nFirst, let’s predict the values. Predict also gives us values on the log odds scale, so, some transformation is needed:\n\npredictedmodel&lt;-predict.glm(codmodel1,cod,se.fit = T)\nci_lwr &lt;- with(predictedmodel, plogis(fit - 1.96*se.fit))\nci_upr &lt;- with(predictedmodel, plogis(fit + 1.96*se.fit))\ncod2&lt;-cbind(cod,predictedmodel)\ncod2$fit2&lt;-plogis(cod2$fit)\ncod2$lwr&lt;-ci_lwr\ncod2$upr&lt;-ci_upr\n\nAnd finally, let’s plot it!\nThese type of models can be pretty tough to plot. One option is to use a grid, in which each column is a year, and each row is an area:\n\nggplot(cod2,aes(x=Length,y=Prevalence,ymin=lwr,ymax=upr))+\n  geom_point()+\n  geom_line(aes(y=fit2))+\n  geom_ribbon(alpha=0.15)+\n  xlab(\"Length (cm)\")+\n  ylab(\"Prevalence\")+\n  theme_bw()+\n  facet_grid(Area~Year)\n\n\n\n\n\n\n\n\nWhile tough to interpret, we can see some patterns. For example, fish from 2001 have a lower prevalence probability!\nAnother way to plot it, is to choose one categorical variable to be presented as colors, and the other one to be at the grid level:\n\nggplot(cod2,aes(x=Length,y=Prevalence,ymin=lwr,ymax=upr,color=Area,shape=Area,fill=Area))+\n  geom_point()+\n  geom_line(aes(y=fit2))+\n  geom_ribbon(alpha=0.1)+\n  xlab(\"Length (cm)\")+\n  ylab(\"Prevalence\")+\n  theme_bw()+\n  facet_grid(~Year)\n\n\n\n\n\n\n\n\nIn here, we can see two patterns: 2001 has lower probability of prevalence, while area 4 has a higher probability. Finally, size also has an effect.\n\n\n\n\n\n\nAssignment question 3📝\n\n\n\nLook at the cod data and come up with 3 “biological hypotheses” that you can run as models. Run the three models\nUse model selection or any method that you want to compare the 4 models (the model I ran; codmodel1; and the models you ran). And select the best model.\n\n\n\n\n\n\n\n\nAssignment question 4 📝\n\n\n\nFor the best model from question 2 (if the best model is the one I ran, then select the second best model) please do the following:\n\nInterpret the summary output\nRun an ANOVA (and if needed) a pairwise comparison\nplot the model. If the model seems too complex to plot, ask for my help, and we can figure it out together 😃",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear models</span>"
    ]
  },
  {
    "objectID": "assignment5.html#testing-your-knowledge",
    "href": "assignment5.html#testing-your-knowledge",
    "title": "5  Generalized Linear models",
    "section": "5.3 Testing your knowledge",
    "text": "5.3 Testing your knowledge\nMoving forward, for each assignment I will ask you 1 or 2 questions in which you will have to apply knowledge from previous classes and assignments.\n\n\n\n\n\n\nAssignment question 5 📝\n\n\n\nData: welldata.csv\n1. wellID - ID of well\n2. fluoride - mg/L of fluoride in a sample\n\nResearch question:\nYou are tasked with researching whether the mean content of fluoride in a large rural area with &gt;2,000 private wells might be over the EPA recommendation of 4.0 mg/L. You sample 28 wells.\n\n\n\n\n\n\n\n\nAssignment question 6 📝\n\n\n\nData: parasitecod.csv\n1. Intensity: Number of parasites present\n2. Prevalence: 1-parasite present, 0- no parasite\n3. Year\n4. Depth (in meters)\n5. Weight (g)\n6.Length (cm)\n7. Sex\n8. Stage\n9. Age\n10. Area\n\nBe aware! The weight, length, stage and age covariates are highly co-linear! Use only one at a time.\nExplore 3 different hypotheses (make sure to include a null model as part of your hypotheses), but use intensity as your response variable.\nBe aware! The intensity response variable is missing some data! Thee are cases were the number of parasites were not counted. You need to deal with this before you can run the models.",
    "crumbs": [
      "Home",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalized Linear models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Box, George E. P., and Norman Richard Draper. 1987. Empirical\nmodel-building and response surfaces. 7. Dr. Wiley series in\nprobability and mathematical statistics. Applied probability and\nstatistics. New York: Wiley.\n\n\nBurnham, Kenneth P., and David Ray Anderson. 2010. Model selection\nand multimodel inference: a practical information-theoretic\napproach. 2. ed. New York, NY: Springer.\n\n\nChilds, Dylan. 2024. “Chapter 10 Working Directories and Data\nFiles.” In. https://dzchilds.github.io/eda-for-bio/working-directories-and-data-files.html.\n\n\nScott, James. n.d. Data Science in r: A Gentle Introduction. https://bookdown.org/jgscott/DSGI/.\n\n\nTredennick, Andrew T., Giles Hooker, Stephen P. Ellner, and Peter B.\nAdler. 2021. “A Practical Guide to Selecting Models for\nExploration, Inference, and Prediction in Ecology.”\nEcology 102 (6): e03336. https://doi.org/10.1002/ecy.3336.\n\n\nWard, Caitlin, and Collin Nolte. 2024. An Intuitive, Interactive,\nIntroduction to Biostatistics. https://collinn.github.io/sbi/index.html#acknowledgements.\n\n\nWickham, Hadley. 2014. “Tidy Data.” The Journal of\nStatistical Software 59 (10). http://www.jstatsoft.org/v59/i10/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science: Import, Tidy, Transform, Visualize, and Model\nData. Second edition. Beijing ; Sebastopol, CA: O’Reilly.",
    "crumbs": [
      "Home",
      "References"
    ]
  },
  {
    "objectID": "installation_instructions.html",
    "href": "installation_instructions.html",
    "title": "Appendix A — Installing R and RStudio",
    "section": "",
    "text": "A.1 Install R and RStudio",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "installation_instructions.html#install-r-and-rstudio",
    "href": "installation_instructions.html#install-r-and-rstudio",
    "title": "Appendix A — Installing R and RStudio",
    "section": "",
    "text": "A.1.1 Install R\nR can be downloaded from the following page https://cran.r-project.org/, follow the instructions to download it.\n\nA.1.1.1 Windows\nTo install R on Windows, click the “Download R for Windows” link. Then click the “base” link. Next, click the first link at the top of the new page.\n\n\nA.1.1.2 Mac\nTo install R in a Mac the “Download R for Mac” link. Next, click on the R-4.4.2 package link.\n\n\nA.1.1.3 Chromebook\nChromebooks don’t allow you to install R or the RStudio program. However, there is an online platform that will allow you to run RStudio. If you go to https://posit.cloud , you should be able to create an account, and in your workspace you can open a new RStudio project. Your window will look the same as what we are working with, though there may be some quirks to getting\n\n\n\nA.1.2 Install RStudio\nGo to https://posit.co/download/rstudio-desktop/\nClick on the Download RStudio Desktop link. If you are using a Mac, you will need to scroll down a bit to see the download link.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "installation_instructions.html#explore-r-and-rstudio",
    "href": "installation_instructions.html#explore-r-and-rstudio",
    "title": "Appendix A — Installing R and RStudio",
    "section": "A.2 Explore R and RStudio",
    "text": "A.2 Explore R and RStudio\nRStudio is a program, not different than Word. It works as a “wrapper” or “editor”. You will write the code in RStudio, and it will run it in program R. You will rarely or maybe never open program R. You will do everything from RStudio!\n\n\n\n\n\n\nTip\n\n\n\nRStudio needs R to function, so you need to download both programs!\n\n\nAfter you open RStudio, this is what you will see:\n\n\n\nRStudio Window\n\n\nNow, you can code in R!\n\nA.2.1 Let’s check R and RStudio!\nLet’s do a couple of things in R. Run the following lines (or similar, you can use different numbers!) on the Console (big screen on the left!)\nFirst, let’s do basic math:\n\n5+5\n10*7\n89-5\n90/3\n\nAs you can see, we can use R as a calculator\nNow, let’s create a vector\n\n1:10\n\n\n120:170\n\n\n\n\n\n\n\nTry it! ✏️\n\n\n\nNow, try to create a vector from 200 to 300\n\n\nFinally, let’s roll a die!\nWe will roll a “D20” die (this is a die with 20 sides). First, we need to create this die. We can create objects in r using the following symbol: &lt;- . Objects will save data, and we can use the object name to extract the data. We can create our dice using the following:\n\\[\n\\underbrace{D20}_{Object\\; name} \\; \\; \\; \\underbrace{&lt;-}_{arrow} \\; \\; \\underbrace{1:20}_{Data}\n\\]\nThe arrow is created with these two symbols: &lt; and - .\n\nD20&lt;-1:20\n\nNow, if you do the following:\n\nD20\n\nYou extract the data from the object. Pretty cool!\nNow, let’s roll our die!\n\nsample(x=D20, size=1)\n\nLet’s now roll 3 D20 dice:\n\nsample(x=D20, size=3, replace=TRUE)\n\n\n\n\n\n\n\nThink about it! 🧠\n\n\n\nWhat do you think the replace=TRUE does?",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Installing R and RStudio</span>"
    ]
  },
  {
    "objectID": "Datawrangling.html",
    "href": "Datawrangling.html",
    "title": "Appendix B — Data Wrangling",
    "section": "",
    "text": "B.1 Additional tidy packages\nData wrangling might be very important for you in your career. While this topic doesn’t fall neatly in any of our topics, I usually decide to have a break around week 5 and look at this.\nThis assignment was written based on the following literature:\nI hope this is a useful assignment for you, and I highly recommend you check all of those tools! Hadley Wickham is the chief scientist at RStudio and he is the creator of ggplot, dplyr, and many other great tools.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. The tidyverse is “opinionated” in that it is fairly rigid about input types and output types, and most functions are designed to accomplish one specific task. This makes it easier for users to avoid some of the sneakier bugs that are hard to notice (for example: different output types of the apply functions). It also tends to make analyses more reproducible.\nThe packages are:\nYou can install all of them using the following:\nYou can also load ALL of them using the following:\nWhile loading all the packages at once is useful, I recommend you load the specific packages you want!\nI also recommend you use the package lubridate if you ever work with dates!",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "Datawrangling.html#my-thoughts",
    "href": "Datawrangling.html#my-thoughts",
    "title": "Appendix B — Data Wrangling",
    "section": "B.2 My thoughts",
    "text": "B.2 My thoughts\nBefore continuing I do want to admit to the fact that I generally prefer a combination of baseR and the tidyverse, and I will continue supplying code in baseR. I also really enjoy using data.table which is faster and has some great functionality, and I recommend you check it out after you master tidy. However, I do recognize that a majority of users prefer the tidyverse, and have fully transitioned to using it.\nParticularly, most users find that the syntax can be easier to understand and write, as it is written from left to right (instead of inside-out). You are free to use any package or any syntax that works best for you.\nWhile early in the semester I had some sections in which I expected you to run some specific code (with the idea of showcasing some of baseR syntax), going forward the focus will be in the results. If you identify the analysis needed, and you do it, it will be correct! No matter hoe you get there!\n\n\n\n\n\n\nTibble, data.frame, what’s the difference?\n\n\n\nI don’t want you to focus on this too much. These two types of objects have different structures, and the tidyverse uses tibble. There isn’t anything too fundamentally different between them, so for the time being, just think of them as equal. I will continue to refer to them as data.frame",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  },
  {
    "objectID": "Datawrangling.html#data-wrangling",
    "href": "Datawrangling.html#data-wrangling",
    "title": "Appendix B — Data Wrangling",
    "section": "B.3 Data Wrangling",
    "text": "B.3 Data Wrangling\nThere are three main parts for data analysis:\n\nManaging (or Wrangling) the data\nAnalyzing it (vi visualization and modeling)\nCommunicating the results\n\nI like this figure from the first edition of R for Data Science: https://r4ds.had.co.nz/index.html\n\n\n\nData wrangling (read R for Data Science)\n\n\nWe have been working on visualizing and modeling data (mostly modeling though). Now, we need to focus on wrangling data.\nAs you see in the image, there are three main important sections when wrangling data:\n\nImport\nTidy\nTransform\n\n\nB.3.1 Import\n\n\n\n\n\n\nread.csv vs read_csv\n\n\n\nIn the last ~5 years there has been a migration from read.csv() to read_csv(). Main differences are: 1) read.csv() doesn’t require a package and creates a data.frame, while read_csv() creates a tibble\n\n\nYou have so far been importing data using the read.csv function by saving the file in the same directory as the quarto file. However you should be able to do all of the following:\n\nCreate a project and load files from within the project\nAdd a folder to the project and read files from the folder, not the root of the directory\nOpen an R script (not quarto, not in a project) and import a dataset\nChange the working directory and check the working directory from within R\n\nIf you can do all of that, congrats! You have 1/3 of the skills needed to wrangle data in R 😃 and you can skip the first assignment question.\nIf you don’t know how to do these things, here is your first assignment question: go to https://www.statology.org/import-csv-into-r/ and follow the first two methods and make sure you can do all these things\n\n\nB.3.2 Tidy and transform\nLet’s talk about probably the biggest change using tidy.\nFor this we will use a “pre-loaded” data set in R. The iris dataset. This dataset is included with baseR and you can call it using the data() function:\n\ndata(iris)\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nIf we wanted to get a dataframe with only setosa species, add a new column where we multiply the width of the pedal by 10 (to change units) we would do the following in baseR:\n\nSetosa&lt;-iris[iris$Species==\"setosa\",]\nSetosa$widthm&lt;-Setosa$Sepal.Width*10\nmean(Setosa$widthm)\n\n[1] 34.28\n\n\nIf we used the tidyverse\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nWe would do the following:\n\niris %&gt;% filter(Species==\"setosa\") %&gt;% mutate(widthm = Sepal.Width*10) \n\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species widthm\n1           5.1         3.5          1.4         0.2  setosa     35\n2           4.9         3.0          1.4         0.2  setosa     30\n3           4.7         3.2          1.3         0.2  setosa     32\n4           4.6         3.1          1.5         0.2  setosa     31\n5           5.0         3.6          1.4         0.2  setosa     36\n6           5.4         3.9          1.7         0.4  setosa     39\n7           4.6         3.4          1.4         0.3  setosa     34\n8           5.0         3.4          1.5         0.2  setosa     34\n9           4.4         2.9          1.4         0.2  setosa     29\n10          4.9         3.1          1.5         0.1  setosa     31\n11          5.4         3.7          1.5         0.2  setosa     37\n12          4.8         3.4          1.6         0.2  setosa     34\n13          4.8         3.0          1.4         0.1  setosa     30\n14          4.3         3.0          1.1         0.1  setosa     30\n15          5.8         4.0          1.2         0.2  setosa     40\n16          5.7         4.4          1.5         0.4  setosa     44\n17          5.4         3.9          1.3         0.4  setosa     39\n18          5.1         3.5          1.4         0.3  setosa     35\n19          5.7         3.8          1.7         0.3  setosa     38\n20          5.1         3.8          1.5         0.3  setosa     38\n21          5.4         3.4          1.7         0.2  setosa     34\n22          5.1         3.7          1.5         0.4  setosa     37\n23          4.6         3.6          1.0         0.2  setosa     36\n24          5.1         3.3          1.7         0.5  setosa     33\n25          4.8         3.4          1.9         0.2  setosa     34\n26          5.0         3.0          1.6         0.2  setosa     30\n27          5.0         3.4          1.6         0.4  setosa     34\n28          5.2         3.5          1.5         0.2  setosa     35\n29          5.2         3.4          1.4         0.2  setosa     34\n30          4.7         3.2          1.6         0.2  setosa     32\n31          4.8         3.1          1.6         0.2  setosa     31\n32          5.4         3.4          1.5         0.4  setosa     34\n33          5.2         4.1          1.5         0.1  setosa     41\n34          5.5         4.2          1.4         0.2  setosa     42\n35          4.9         3.1          1.5         0.2  setosa     31\n36          5.0         3.2          1.2         0.2  setosa     32\n37          5.5         3.5          1.3         0.2  setosa     35\n38          4.9         3.6          1.4         0.1  setosa     36\n39          4.4         3.0          1.3         0.2  setosa     30\n40          5.1         3.4          1.5         0.2  setosa     34\n41          5.0         3.5          1.3         0.3  setosa     35\n42          4.5         2.3          1.3         0.3  setosa     23\n43          4.4         3.2          1.3         0.2  setosa     32\n44          5.0         3.5          1.6         0.6  setosa     35\n45          5.1         3.8          1.9         0.4  setosa     38\n46          4.8         3.0          1.4         0.3  setosa     30\n47          5.1         3.8          1.6         0.2  setosa     38\n48          4.6         3.2          1.4         0.2  setosa     32\n49          5.3         3.7          1.5         0.2  setosa     37\n50          5.0         3.3          1.4         0.2  setosa     33\n\n\nSo, there are two main differences. tidy reads “left to right”, so, you filter the data, and the mutate it. But maybe the main difference is the use of a piping operator %&gt;%.\nR pipes are a way to chain multiple operations together in a concise and expressive way. They are so popular thar R introduced them to their base use as: |&gt;.\nNow, imagine you want to\n1) add the widthm column to the original dataset\n2) estimate all of the following for each of the three species and each of the columns:\nMean, median, maximum value, and minimum value.\nThat seems like a lot of coding using baseR! However, using pipes and the powerful dplyr, we can do the following:\n\niris %&gt;% group_by(Species) %&gt;% mutate(widthm = Sepal.Width*10) %&gt;% summarise_all(list(mean=mean,median=median,max=max,min=min))\n\n# A tibble: 3 × 21\n  Species  Sepal.Length_mean Sepal.Width_mean Petal.Length_mean Petal.Width_mean\n  &lt;fct&gt;                &lt;dbl&gt;            &lt;dbl&gt;             &lt;dbl&gt;            &lt;dbl&gt;\n1 setosa                5.01             3.43              1.46            0.246\n2 versico…              5.94             2.77              4.26            1.33 \n3 virgini…              6.59             2.97              5.55            2.03 \n# ℹ 16 more variables: widthm_mean &lt;dbl&gt;, Sepal.Length_median &lt;dbl&gt;,\n#   Sepal.Width_median &lt;dbl&gt;, Petal.Length_median &lt;dbl&gt;,\n#   Petal.Width_median &lt;dbl&gt;, widthm_median &lt;dbl&gt;, Sepal.Length_max &lt;dbl&gt;,\n#   Sepal.Width_max &lt;dbl&gt;, Petal.Length_max &lt;dbl&gt;, Petal.Width_max &lt;dbl&gt;,\n#   widthm_max &lt;dbl&gt;, Sepal.Length_min &lt;dbl&gt;, Sepal.Width_min &lt;dbl&gt;,\n#   Petal.Length_min &lt;dbl&gt;, Petal.Width_min &lt;dbl&gt;, widthm_min &lt;dbl&gt;\n\n\n\n\nB.3.3 Wide and long data\nOne of the most important transformation is changing your data from wide to long format (or long to wide).\nWhile we generally have this structure in our data:\n\n\n\nVariable, observations, and values\n\n\nIf we have some repeated measures (think, an individual gets measured weekly), we can have each week be a column, or we can have a column named “week” and a column for the “value”.\nThink about the following example: You are traveling to four sites and measuring the diameter at breast height for 3 species of tree. You do this at four sites. There are two ways you can present these data:\n\n\n\nWide and Long data\n\n\nBoth can be useful! And we can go back and from using R.\n\ntree&lt;-read.csv(\"data/trees.csv\")\nhead(tree)\n\n       Species Site.A Site.B Site.C Site.D\n1  Acer rubrum     15      8     30     27\n2 Quercus alba     29     17     14     42\n3  Pinus taeda     10     19     25     23\n\n\nLet’s make the dataset longer! We need to make all columns (except species) into two columns, one for the site, one for the value:\n\ntreelong&lt;- tree%&gt;% pivot_longer(!Species,names_to = \"site\", values_to = \"meanDBH\")\nhead(treelong)\n\n# A tibble: 6 × 3\n  Species      site   meanDBH\n  &lt;chr&gt;        &lt;chr&gt;    &lt;int&gt;\n1 Acer rubrum  Site.A      15\n2 Acer rubrum  Site.B       8\n3 Acer rubrum  Site.C      30\n4 Acer rubrum  Site.D      27\n5 Quercus alba Site.A      29\n6 Quercus alba Site.B      17\n\n\nAnd let’s make this “long” dataset into a wide form:\n\ntreewide&lt;- treelong%&gt;% pivot_wider(names_from = \"site\", values_from = \"meanDBH\")\ntreewide\n\n# A tibble: 3 × 5\n  Species      Site.A Site.B Site.C Site.D\n  &lt;chr&gt;         &lt;int&gt;  &lt;int&gt;  &lt;int&gt;  &lt;int&gt;\n1 Acer rubrum      15      8     30     27\n2 Quercus alba     29     17     14     42\n3 Pinus taeda      10     19     25     23\n\n\nEasy! This will be useful for you in the future!\nPlease read careful before starting to work on the assignment: For this week’s assignment, you need to go to: https://bookdown.org/jgscott/DSGI/data-wrangling.html and read chapter 6. Reality is, this “tutorial” and this book as well as the R for Data Science book are better written and have better information than what I could do for this class, and I think it will be more useful to you!. If you are new to R and the tidyverse your assignment I recommend you open a new r script and follow the “tutorial”. You can ask me questions. The assignment will be to just let me know in Canvas whether you like this way (or baseR) better. If you are experienced in tidy, and you know the basics, then, I recommend you read chapter 17 https://bookdown.org/jgscott/DSGI/probability-models.html. (Actually, everyone should read it, if you have time). If you understand probability, then you understand statistics, whether they are frequentist, Bayesian, multivariate, etc. You can also read any other chapter if you find it more useful. Your assignment is telling me what chapter you worked on.\n\n\n\n\n\nScott, James. n.d. Data Science in r: A Gentle Introduction. https://bookdown.org/jgscott/DSGI/.\n\n\nWickham, Hadley. 2014. “Tidy Data.” The Journal of Statistical Software 59 (10). http://www.jstatsoft.org/v59/i10/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Second edition. Beijing ; Sebastopol, CA: O’Reilly.",
    "crumbs": [
      "Home",
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Data Wrangling</span>"
    ]
  }
]